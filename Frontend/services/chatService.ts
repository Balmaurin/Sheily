import axios from 'axios';
import { toast } from "@/components/ui/use-toast";

interface ChatResponse {
  response: string;
  model: string;
  timestamp: number;
}

interface ChatRequestParams {
  prompt: string;
  max_length?: number;
  temperature?: number;
}

const BACKEND_URL = process.env.NEXT_PUBLIC_BACKEND_URL || 'http://localhost:8000';
const SHEILY_GATEWAY_URL = 'http://localhost:8080'; // Gateway Sheily AI
const LLM_SERVER_URL = 'http://localhost:8005'; // LLM Server directo (solo para emergencias)

// Endpoint correcto: Dashboard ‚Üí Gateway Sheily AI ‚Üí LLM Server
const CHAT_ENDPOINT = `${SHEILY_GATEWAY_URL}/query`;

export const chatService = {
  handleApiError(error: any, defaultMessage: string = "Error en la operaci√≥n"): void {
    if (error.response) {
      toast({
        title: "Error de Chat",
        description: `${defaultMessage}: ${error.response.data?.message || error.response.status}`,
        variant: "destructive"
      });
    } else if (error.request) {
      toast({
        title: "Error de Red",
        description: "No se recibi√≥ respuesta del servidor de chat",
        variant: "destructive"
      });
    } else {
      toast({
        title: "Error de Configuraci√≥n",
        description: error.message || defaultMessage,
        variant: "destructive"
      });
    }
  },

  async sendMessage(params: ChatRequestParams): Promise<{
    message: string;
    model_used: string;
    response_time: number;
    tokens_used: number;
  }> {
    try {
      const startTime = Date.now();

      // Conectar con el Gateway Sheily AI que maneja el LLM
      const response = await axios.post(CHAT_ENDPOINT, {
        query: params.prompt,
        domain: "general",
        max_tokens: params.max_length || 500,
        temperature: params.temperature || 0.7,
        top_p: 0.9
      }, {
        timeout: 30000, // 30 segundos timeout
        headers: {
          'Content-Type': 'application/json'
        }
      });

      const endTime = Date.now();
      const responseTime = (endTime - startTime) / 1000;

      // Verificar que tenemos una respuesta del gateway
      if (!response.data || !response.data.response) {
        throw new Error('El gateway Sheily AI no devolvi√≥ una respuesta v√°lida');
      }

      console.log('‚úÖ Respuesta del Gateway Sheily AI:', {
        gateway: 'Sheily AI Gateway',
        llm_model: response.data.model_used,
        response_time: response.data.response_time,
        confidence: response.data.confidence,
        domain: response.data.domain,
        tokens_used: response.data.tokens_used,
        quality_score: response.data.quality_score
      });

      // Retornar respuesta del gateway con m√©tricas detalladas
      return {
        message: response.data.response,
        model_used: response.data.model_used || 'Llama-3.2-3B-Instruct-Q8_0',
        response_time: response.data.response_time || responseTime,
        tokens_used: response.data.tokens_used || Math.ceil(response.data.response.length / 4)
      };
    } catch (error: any) {
      console.error('‚ùå Error conectando con Gateway Sheily AI:', error);

      // Manejar errores espec√≠ficos del gateway
      if (error.response?.status === 503) {
        chatService.handleApiError(error, "El servidor LLM no est√° disponible. Por favor, intenta m√°s tarde.");
      } else if (error.response?.status === 504) {
        chatService.handleApiError(error, "El servidor tard√≥ demasiado en responder. Por favor, intenta con una consulta m√°s corta.");
      } else if (error.code === 'ECONNREFUSED') {
        chatService.handleApiError(error, "No se puede conectar con el Gateway Sheily AI. Verifica que est√© ejecut√°ndose.");
      } else {
        chatService.handleApiError(error, "Error de conexi√≥n con el Gateway Sheily AI");
      }

      throw error; // No fallbacks - el gateway maneja todo
    }
  },

  async checkModelHealth(): Promise<boolean> {
    try {
      // Verificar el Gateway Sheily AI
      const response = await axios.get(`${SHEILY_GATEWAY_URL}/health`);
      const isHealthy = response.status === 200 && response.data.status !== 'degraded';

      console.log('üîç Estado del Gateway Sheily AI:', {
        status: response.data.status,
        uptime: response.data.uptime,
        requests_processed: response.data.requests_processed,
        llm_connected: response.data.connections?.llm_server,
        backend_connected: response.data.connections?.backend,
        healthy: isHealthy
      });

      return isHealthy;
    } catch (error) {
      console.error('‚ùå Gateway Sheily AI no disponible:', error);
      chatService.handleApiError(error, "Gateway Sheily AI no disponible");
      return false;
    }
  },

  async getModelInfo() {
    try {
      // Obtener informaci√≥n actual del gateway
      const statusResponse = await axios.get(`${SHEILY_GATEWAY_URL}/status`);

      return {
        name: "Sheily AI Gateway",
        subtitle: "Sistema inteligente con Llama-3.2-3B-Instruct-Q8_0",
        purpose: "Gateway inteligente que conecta el dashboard con el modelo Llama 3.2, proporcionando respuestas contextuales y de alta calidad a trav√©s de una arquitectura optimizada.",
        capabilities: [
          "Procesamiento inteligente de consultas v√≠a Gateway",
          "Clasificaci√≥n autom√°tica de dominios",
          "Gesti√≥n optimizada de conexiones LLM",
          "M√©tricas detalladas de rendimiento",
          "Sistema de calidad de respuestas",
          "Arquitectura de microservicios escalable"
        ],
        backend_model: statusResponse.data.connections?.llm_server?.model || "Llama-3.2-3B-Instruct-Q8_0",
        quantization: "Q8_0",
        parameters: "3B par√°metros",
        memory: "~2.2GB VRAM",
        context: "4096 tokens",
        gateway_status: statusResponse.data.status,
        connections: statusResponse.data.connections,
        special_features: [
          "Arquitectura Gateway optimizada",
          "Conexi√≥n directa con LLM Server",
          "Procesamiento inteligente de dominios",
          "M√©tricas de calidad y rendimiento",
          "Gesti√≥n autom√°tica de errores",
          "Logging avanzado de consultas"
        ]
      };
    } catch (error) {
      console.error('Error obteniendo informaci√≥n del Gateway Sheily AI:', error);

      // Fallback con informaci√≥n b√°sica si el gateway no est√° disponible
      return {
        name: "Sheily AI Gateway",
        subtitle: "Sistema inteligente con Llama-3.2-3B-Instruct-Q8_0",
        purpose: "Gateway inteligente que conecta el dashboard con el modelo Llama 3.2",
        capabilities: [
          "Procesamiento inteligente de consultas",
          "Clasificaci√≥n autom√°tica de dominios",
          "M√©tricas de rendimiento",
          "Sistema de calidad de respuestas"
        ],
        backend_model: "Llama-3.2-3B-Instruct-Q8_0",
        quantization: "Q8_0",
        parameters: "3B par√°metros",
        memory: "~2.2GB VRAM",
        context: "4096 tokens",
        gateway_status: "unavailable",
        special_features: [
          "Arquitectura Gateway optimizada",
          "Conexi√≥n con LLM Server",
          "Gesti√≥n autom√°tica de errores"
        ]
      };
    }
  }
};

export default chatService;
