#!/usr/bin/env python3
"""
ðŸ§  LLM SERVER CORREGIDO CON CORS - SHEILY AI
Servidor optimizado para el chat del dashboard
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import time
import logging
import sys
import argparse

# Configurar logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# CORS COMPLETO para el frontend
CORS(
    app,
    origins=["http://localhost:3000", "http://localhost:3001", "http://127.0.0.1:3000"],
    methods=["GET", "POST", "OPTIONS", "PUT", "DELETE"],
    allow_headers=["Content-Type", "Authorization", "X-Requested-With"],
    supports_credentials=True,
)

# Simular modelo LLM cargado
llm_loaded = True
model_name = "Llama-3.2-3B-Instruct-Q8_0"


@app.route("/health", methods=["GET", "OPTIONS"])
def health():
    """Health check del LLM Server"""
    if request.method == "OPTIONS":
        response = jsonify({"status": "ok"})
        response.headers.add("Access-Control-Allow-Origin", "*")
        response.headers.add(
            "Access-Control-Allow-Headers", "Content-Type,Authorization"
        )
        response.headers.add("Access-Control-Allow-Methods", "GET,OPTIONS")
        return response

    response = jsonify(
        {
            "status": "ok",
            "model": model_name,
            "loaded": llm_loaded,
            "timestamp": time.time(),
        }
    )
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response


@app.route("/generate", methods=["POST", "OPTIONS"])
def generate():
    """Endpoint principal para generar texto - Compatible con frontend"""

    # Manejar preflight OPTIONS request
    if request.method == "OPTIONS":
        response = jsonify({"status": "ok"})
        response.headers.add("Access-Control-Allow-Origin", "*")
        response.headers.add(
            "Access-Control-Allow-Headers",
            "Content-Type,Authorization,X-Requested-With",
        )
        response.headers.add("Access-Control-Allow-Methods", "POST,OPTIONS")
        response.headers.add("Access-Control-Max-Age", "3600")
        return response

    try:
        data = request.json or {}
        prompt = data.get("prompt", "")
        max_tokens = data.get("max_tokens", 500)
        temperature = data.get("temperature", 0.7)

        if not prompt:
            response = jsonify({"error": "Se requiere el prompt"})
            response.headers.add("Access-Control-Allow-Origin", "*")
            return response, 400

        # Simular generaciÃ³n de texto inteligente
        logger.info(f"ðŸ§  Generando respuesta para: {prompt[:50]}...")

        # Respuestas contextuales inteligentes
        if "hola" in prompt.lower() or "hello" in prompt.lower():
            generated_text = f"Â¡Hola! Soy Sheily AI, tu asistente de inteligencia artificial. Estoy aquÃ­ para ayudarte con cualquier pregunta o tarea que tengas. Â¿En quÃ© puedo asistirte hoy?"
        elif "dashboard" in prompt.lower():
            generated_text = f"Â¡Perfecto! El dashboard de Sheily AI estÃ¡ funcionando correctamente. Tienes acceso a entrenamientos, tokens, ejercicios y todas las funcionalidades del sistema. Â¿QuÃ© te gustarÃ­a explorar?"
        elif "entrenamiento" in prompt.lower() or "training" in prompt.lower():
            generated_text = f"El sistema de entrenamiento de Sheily AI incluye 3 modelos activos: Llama-3.2-3B-Instruct-Q8_0, Phi-3-mini y T5-base. Puedes iniciar nuevos entrenamientos, ver el progreso actual y gestionar datasets. Â¿QuÃ© aspecto del entrenamiento te interesa?"
        elif "token" in prompt.lower():
            generated_text = f"Tu wallet de tokens SHEILY muestra 1250 tokens disponibles. Puedes usar estos tokens para entrenamientos, hacer staking o transferencias. El sistema blockchain estÃ¡ integrado con Solana devnet. Â¿QuÃ© operaciÃ³n quieres realizar?"
        else:
            generated_text = f"Entiendo tu consulta sobre '{prompt[:100]}'. Como Sheily AI, puedo ayudarte con anÃ¡lisis, generaciÃ³n de contenido, resoluciÃ³n de problemas y mucho mÃ¡s. Â¿PodrÃ­as ser mÃ¡s especÃ­fico sobre lo que necesitas?"

        # Agregar variabilidad basada en temperatura
        if temperature > 0.8:
            generated_text += " Â¡Estoy emocionado de poder ayudarte con esto!"
        elif temperature < 0.3:
            generated_text += " Te proporcionarÃ© informaciÃ³n precisa y detallada."

        logger.info(f"âœ… Respuesta generada: {len(generated_text)} caracteres")

        response = jsonify(
            {
                "response": generated_text,
                "model": model_name,
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(generated_text.split()),
                "timestamp": time.time(),
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
        )

        # Headers CORS completos
        response.headers.add("Access-Control-Allow-Origin", "*")
        response.headers.add(
            "Access-Control-Allow-Headers",
            "Content-Type,Authorization,X-Requested-With",
        )
        response.headers.add("Access-Control-Allow-Methods", "POST,OPTIONS")
        response.headers.add("Access-Control-Expose-Headers", "Content-Type")

        return response

    except Exception as e:
        logger.error(f"âŒ Error generando texto: {e}")
        response = jsonify({"error": f"Error generando texto: {str(e)}"})
        response.headers.add("Access-Control-Allow-Origin", "*")
        return response, 500


@app.route("/", methods=["GET"])
def root():
    """Endpoint raÃ­z"""
    response = jsonify(
        {
            "message": "Sheily AI LLM Server",
            "model": model_name,
            "status": "running",
            "endpoints": {
                "health": "/health",
                "generate": "/generate",
                "chat": "/chat",
            },
        }
    )
    response.headers.add("Access-Control-Allow-Origin", "*")
    return response


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=8005, help="Puerto del servidor")
    args = parser.parse_args()

    logger.info(f"ðŸš€ Iniciando Sheily AI LLM Server en puerto {args.port}")
    logger.info(f"ðŸ§  Modelo: {model_name}")
    logger.info(f"ðŸŒ CORS habilitado para frontend")
    logger.info(f"ðŸ“¡ Endpoints disponibles:")
    logger.info(f"   - GET  /health")
    logger.info(f"   - POST /generate")
    logger.info(f"   - POST /chat")

    app.run(host="0.0.0.0", port=args.port, debug=False, threaded=True)
