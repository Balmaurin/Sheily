"""
Validador de Modelos - Model Validator
=====================================

Componentes para validaci√≥n y verificaci√≥n de modelos.
"""

import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error, accuracy_score
import joblib
import os

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """Resultado de validaci√≥n del modelo"""

    is_valid: bool
    score: float
    confidence: float
    issues: List[str]
    recommendations: List[str]


@dataclass
class CrossValidationResult:
    """Resultado de validaci√≥n cruzada"""

    mean_score: float
    std_score: float
    fold_scores: List[float]
    cv_folds: int


class ModelValidator:
    """Validador de modelos de machine learning"""

    def __init__(self):
        self.validation_thresholds = {
            "min_accuracy": 0.7,
            "min_cv_score": 0.6,
            "max_cv_std": 0.2,
            "min_confidence": 0.8,
        }

    def validate_model(
        self,
        model: Any,
        X: np.ndarray,
        y: np.ndarray,
        validation_type: str = "cross_validation",
    ) -> ValidationResult:
        """Valida un modelo usando diferentes m√©todos"""
        try:
            logger.info(f"üîÑ Validando modelo con m√©todo: {validation_type}")

            issues = []
            recommendations = []

            if validation_type == "cross_validation":
                cv_result = self._cross_validate_model(model, X, y)
                score = cv_result.mean_score
                confidence = self._calculate_confidence(cv_result)

                if cv_result.mean_score < self.validation_thresholds["min_cv_score"]:
                    issues.append(
                        f"Puntuaci√≥n de validaci√≥n cruzada baja: {cv_result.mean_score:.3f}"
                    )
                    recommendations.append(
                        "Considerar ajustar hiperpar√°metros o usar m√°s datos"
                    )

                if cv_result.std_score > self.validation_thresholds["max_cv_std"]:
                    issues.append(
                        f"Alta variabilidad en validaci√≥n cruzada: {cv_result.std_score:.3f}"
                    )
                    recommendations.append(
                        "El modelo puede ser inestable, considerar regularizaci√≥n"
                    )

            elif validation_type == "holdout":
                score, confidence = self._holdout_validate_model(model, X, y)

                if score < self.validation_thresholds["min_accuracy"]:
                    issues.append(f"Puntuaci√≥n de holdout baja: {score:.3f}")
                    recommendations.append("El modelo necesita mejor entrenamiento")

            else:
                raise ValueError(f"Tipo de validaci√≥n no soportado: {validation_type}")

            # Validaciones adicionales
            additional_issues = self._validate_model_structure(model)
            issues.extend(additional_issues)

            # Determinar si el modelo es v√°lido
            is_valid = (
                len(issues) == 0
                and confidence >= self.validation_thresholds["min_confidence"]
            )

            if is_valid:
                recommendations.append("El modelo es v√°lido y listo para producci√≥n")
            else:
                recommendations.append("El modelo necesita mejoras antes de producci√≥n")

            result = ValidationResult(
                is_valid=is_valid,
                score=score,
                confidence=confidence,
                issues=issues,
                recommendations=recommendations,
            )

            logger.info(
                f"‚úÖ Validaci√≥n completada. V√°lido: {is_valid}, Puntuaci√≥n: {score:.3f}"
            )
            return result

        except Exception as e:
            logger.error(f"‚ùå Error validando modelo: {e}")
            return ValidationResult(
                is_valid=False,
                score=0.0,
                confidence=0.0,
                issues=[f"Error en validaci√≥n: {str(e)}"],
                recommendations=["Revisar configuraci√≥n del modelo"],
            )

    def _cross_validate_model(
        self, model: Any, X: np.ndarray, y: np.ndarray
    ) -> CrossValidationResult:
        """Realiza validaci√≥n cruzada del modelo"""
        try:
            # Configurar validaci√≥n cruzada
            cv_folds = min(
                5, len(X) // 10
            )  # M√°ximo 5 folds, m√≠nimo 10 muestras por fold
            cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

            # Realizar validaci√≥n cruzada
            if hasattr(model, "predict_proba"):
                # Para clasificaci√≥n
                scores = cross_val_score(model, X, y, cv=cv, scoring="accuracy")
            else:
                # Para regresi√≥n
                scores = cross_val_score(model, X, y, cv=cv, scoring="r2")

            return CrossValidationResult(
                mean_score=np.mean(scores),
                std_score=np.std(scores),
                fold_scores=scores.tolist(),
                cv_folds=cv_folds,
            )

        except Exception as e:
            logger.error(f"‚ùå Error en validaci√≥n cruzada: {e}")
            return CrossValidationResult(0.0, 0.0, [], 0)

    def _holdout_validate_model(
        self, model: Any, X: np.ndarray, y: np.ndarray
    ) -> Tuple[float, float]:
        """Realiza validaci√≥n holdout del modelo"""
        try:
            from sklearn.model_selection import train_test_split

            # Dividir datos
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Entrenar modelo
            model.fit(X_train, y_train)

            # Predicciones
            y_pred = model.predict(X_test)

            # Calcular m√©tricas
            if hasattr(model, "predict_proba"):
                score = accuracy_score(y_test, y_pred)
            else:
                score = max(0, 1 - mean_squared_error(y_test, y_pred))

            # Calcular confianza (simplificado)
            confidence = min(1.0, score + 0.1)

            return score, confidence

        except Exception as e:
            logger.error(f"‚ùå Error en validaci√≥n holdout: {e}")
            return 0.0, 0.0

    def _validate_model_structure(self, model: Any) -> List[str]:
        """Valida la estructura del modelo"""
        issues = []

        try:
            # Verificar que el modelo tenga m√©todo predict
            if not hasattr(model, "predict"):
                issues.append("El modelo no tiene m√©todo predict")

            # Verificar que el modelo est√© entrenado
            if hasattr(model, "coef_") and model.coef_ is None:
                issues.append(
                    "El modelo no est√° entrenado (coeficientes no disponibles)"
                )

            if (
                hasattr(model, "feature_importances_")
                and model.feature_importances_ is None
            ):
                issues.append(
                    "El modelo no est√° entrenado (importancias no disponibles)"
                )

            # Verificar tama√±o del modelo (simplificado)
            model_size = self._estimate_model_size(model)
            if model_size > 1000000:  # 1MB
                issues.append(f"El modelo es muy grande: {model_size/1000000:.1f}MB")

            return issues

        except Exception as e:
            logger.error(f"‚ùå Error validando estructura del modelo: {e}")
            return ["Error validando estructura del modelo"]

    def _estimate_model_size(self, model: Any) -> int:
        """Estima el tama√±o del modelo en bytes"""
        try:
            # Guardar modelo temporalmente
            temp_path = "temp_model.joblib"
            joblib.dump(model, temp_path)

            # Obtener tama√±o del archivo
            size = os.path.getsize(temp_path)

            # Limpiar archivo temporal
            os.remove(temp_path)

            return size

        except Exception as e:
            logger.error(f"‚ùå Error estimando tama√±o del modelo: {e}")
            return 0

    def _calculate_confidence(self, cv_result: CrossValidationResult) -> float:
        """Calcula la confianza basada en los resultados de validaci√≥n cruzada"""
        try:
            # Basado en la puntuaci√≥n media y la estabilidad
            mean_score = cv_result.mean_score
            std_score = cv_result.std_score

            # Confianza alta si puntuaci√≥n alta y baja variabilidad
            confidence = mean_score * (1 - std_score)
            return max(0.0, min(1.0, confidence))

        except Exception as e:
            logger.error(f"‚ùå Error calculando confianza: {e}")
            return 0.5

    def validate_data_compatibility(
        self, model: Any, X: np.ndarray
    ) -> ValidationResult:
        """Valida la compatibilidad de los datos con el modelo"""
        try:
            issues = []
            recommendations = []

            # Verificar dimensiones
            if hasattr(model, "n_features_in_"):
                expected_features = model.n_features_in_
                actual_features = X.shape[1] if len(X.shape) > 1 else 1

                if expected_features != actual_features:
                    issues.append(
                        f"Incompatibilidad de caracter√≠sticas: esperadas {expected_features}, obtenidas {actual_features}"
                    )
                    recommendations.append(
                        "Ajustar el n√∫mero de caracter√≠sticas en los datos de entrada"
                    )

            # Verificar tipos de datos
            if X.dtype not in [np.float32, np.float64, np.int32, np.int64]:
                issues.append("Los datos no son num√©ricos")
                recommendations.append("Convertir datos a tipos num√©ricos")

            # Verificar valores faltantes
            if np.isnan(X).any():
                issues.append("Los datos contienen valores NaN")
                recommendations.append("Limpiar o imputar valores faltantes")

            # Verificar valores infinitos
            if np.isinf(X).any():
                issues.append("Los datos contienen valores infinitos")
                recommendations.append("Limpiar valores infinitos")

            is_valid = len(issues) == 0
            score = 1.0 if is_valid else 0.5
            confidence = 1.0 if is_valid else 0.3

            if is_valid:
                recommendations.append("Los datos son compatibles con el modelo")

            return ValidationResult(
                is_valid=is_valid,
                score=score,
                confidence=confidence,
                issues=issues,
                recommendations=recommendations,
            )

        except Exception as e:
            logger.error(f"‚ùå Error validando compatibilidad de datos: {e}")
            return ValidationResult(
                is_valid=False,
                score=0.0,
                confidence=0.0,
                issues=[f"Error validando datos: {str(e)}"],
                recommendations=["Revisar formato de datos"],
            )

    def generate_validation_report(
        self, model: Any, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, Any]:
        """Genera un reporte completo de validaci√≥n"""
        try:
            # Validaci√≥n del modelo
            model_validation = self.validate_model(model, X, y, "cross_validation")

            # Validaci√≥n de compatibilidad de datos
            data_validation = self.validate_data_compatibility(model, X)

            # Validaci√≥n cruzada detallada
            cv_result = self._cross_validate_model(model, X, y)

            return {
                "model_validation": {
                    "is_valid": model_validation.is_valid,
                    "score": model_validation.score,
                    "confidence": model_validation.confidence,
                    "issues": model_validation.issues,
                    "recommendations": model_validation.recommendations,
                },
                "data_validation": {
                    "is_valid": data_validation.is_valid,
                    "score": data_validation.score,
                    "confidence": data_validation.confidence,
                    "issues": data_validation.issues,
                    "recommendations": data_validation.recommendations,
                },
                "cross_validation": {
                    "mean_score": cv_result.mean_score,
                    "std_score": cv_result.std_score,
                    "fold_scores": cv_result.fold_scores,
                    "cv_folds": cv_result.cv_folds,
                },
                "overall_assessment": {
                    "is_production_ready": model_validation.is_valid
                    and data_validation.is_valid,
                    "overall_score": (model_validation.score + data_validation.score)
                    / 2,
                    "critical_issues": len(
                        [
                            i
                            for i in model_validation.issues + data_validation.issues
                            if "cr√≠tico" in i.lower()
                        ]
                    ),
                },
            }

        except Exception as e:
            logger.error(f"‚ùå Error generando reporte de validaci√≥n: {e}")
            return {"error": str(e)}
