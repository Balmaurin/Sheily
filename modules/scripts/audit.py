#!/usr/bin/env python3
"""
Script de Auditor√≠a de Modelos de Shaili AI
===========================================

Este script audita y verifica el estado de los dos modelos principales:
- Modelo Principal: Shaili Personal Model (4-bit)
- Modelo de Ramas: paraphrase-multilingual-MiniLM-L12-v2 (16-bit)
"""

import os
import sys
import json
import logging
import torch
from pathlib import Path
from typing import Dict, Any, List

# Configurar logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ModelAuditor:
    """
    Auditor de modelos para Shaili AI

    Verifica:
    - Estado de los modelos principales
    - Configuraciones de cuantizaci√≥n
    - Integridad de archivos
    - Rendimiento y memoria
    """

    def __init__(self):
        """Inicializar auditor"""
        self.audit_results = {}
        self.issues = []
        self.warnings = []

    def audit_main_model(self) -> Dict[str, Any]:
        """
        Auditar el modelo principal (Shaili Personal Model)

        Returns:
            Dict[str, Any]: Resultados de la auditor√≠a
        """
        logger.info("üîç Auditar modelo principal: Shaili Personal Model")

        results = {
            "model_name": "Shaili Personal Model",
            "model_path": "models/custom/shaili-personal-model",
            "quantization": "4-bit",
            "status": "UNKNOWN",
            "issues": [],
            "warnings": [],
            "files": {},
            "performance": {},
        }

        model_path = Path(results["model_path"])

        # Verificar existencia del directorio
        if not model_path.exists():
            results["status"] = "MISSING"
            results["issues"].append("Directorio del modelo no encontrado")
            return results

        # Verificar archivos necesarios
        required_files = [
            "config.json",
            "pytorch_model.bin",
            "tokenizer.json",
            "tokenizer_config.json",
        ]

        for file in required_files:
            file_path = model_path / file
            if file_path.exists():
                size = file_path.stat().st_size
                results["files"][file] = {
                    "exists": True,
                    "size_mb": round(size / (1024 * 1024), 2),
                }
            else:
                results["files"][file] = {"exists": False, "size_mb": 0}
                results["issues"].append(f"Archivo faltante: {file}")

        # Verificar configuraci√≥n de cuantizaci√≥n
        quant_config_path = model_path / "quantization_config.json"
        if quant_config_path.exists():
            try:
                with open(quant_config_path, "r") as f:
                    quant_config = json.load(f)
                results["quantization_config"] = quant_config
            except Exception as e:
                results["warnings"].append(
                    f"Error leyendo configuraci√≥n de cuantizaci√≥n: {e}"
                )
        else:
            results["warnings"].append("Configuraci√≥n de cuantizaci√≥n no encontrada")

        # Intentar cargar el modelo
        try:
            sys.path.append(str(Path(__file__).parent.parent.parent))
            from modules.core.model.shaili_model import ShailiBaseModel

            model = ShailiBaseModel(model_id=str(model_path), quantization="4bit")

            results["status"] = "LOADED"
            results["performance"]["device"] = str(model.base_model.device)
            results["performance"]["dtype"] = str(model.base_model.dtype)

            # Verificar par√°metros
            total_params = sum(p.numel() for p in model.base_model.parameters())
            results["performance"]["total_parameters"] = total_params

            logger.info(f"‚úÖ Modelo principal cargado correctamente")

        except Exception as e:
            results["status"] = "ERROR"
            results["issues"].append(f"Error cargando modelo: {str(e)}")
            logger.error(f"‚ùå Error cargando modelo principal: {e}")

        return results

    def audit_branch_model(self) -> Dict[str, Any]:
        """
        Auditar el modelo de ramas (paraphrase-multilingual-MiniLM-L12-v2)

        Returns:
            Dict[str, Any]: Resultados de la auditor√≠a
        """
        logger.info("üîç Auditar modelo de ramas: paraphrase-multilingual-MiniLM-L12-v2")

        results = {
            "model_name": "paraphrase-multilingual-MiniLM-L12-v2",
            "model_path": "models/custom/shaili-personal-model",
            "quantization": "16-bit",
            "status": "UNKNOWN",
            "issues": [],
            "warnings": [],
            "files": {},
            "performance": {},
        }

        model_path = Path(results["model_path"])

        # Verificar existencia del directorio
        if not model_path.exists():
            results["status"] = "MISSING"
            results["issues"].append("Directorio del modelo no encontrado")
            return results

        # Verificar archivos necesarios para modelo principal
        required_files = [
            "config.json",
            "model.safetensors.index.json",
            "tokenizer.json",
            "tokenizer_config.json",
        ]

        for file in required_files:
            file_path = model_path / file
            if file_path.exists():
                size = file_path.stat().st_size
                results["files"][file] = {
                    "exists": True,
                    "size_mb": round(size / (1024 * 1024), 2),
                }
            else:
                results["files"][file] = {"exists": False, "size_mb": 0}
                if file in ["config.json", "pytorch_model.bin"]:
                    results["issues"].append(f"Archivo faltante: {file}")
                else:
                    results["warnings"].append(f"Archivo opcional faltante: {file}")

        # Intentar cargar el modelo
        try:
            from transformers import AutoModel, AutoTokenizer

            tokenizer = AutoTokenizer.from_pretrained(str(model_path))
            model = AutoModel.from_pretrained(str(model_path))

            results["status"] = "LOADED"
            results["performance"]["embedding_dim"] = model.config.hidden_size
            results["performance"][
                "max_seq_length"
            ] = model.config.max_position_embeddings

            # Probar generaci√≥n de embeddings
            test_text = "Texto de prueba para verificar embeddings"
            inputs = tokenizer(
                test_text, return_tensors="pt", padding=True, truncation=True
            )
            with torch.no_grad():
                outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)
                results["performance"]["test_embedding_shape"] = list(embeddings.shape)

            logger.info(f"‚úÖ Modelo de ramas cargado correctamente")

        except Exception as e:
            results["status"] = "ERROR"
            results["issues"].append(f"Error cargando modelo: {str(e)}")
            logger.error(f"‚ùå Error cargando modelo de ramas: {e}")

        return results

    def audit_system_integration(self) -> Dict[str, Any]:
        """
        Auditar la integraci√≥n del sistema

        Returns:
            Dict[str, Any]: Resultados de la auditor√≠a
        """
        logger.info("üîç Auditar integraci√≥n del sistema")

        results = {
            "integration_status": "UNKNOWN",
            "components": {},
            "issues": [],
            "warnings": [],
        }

        # Verificar gestor de ramas
        try:
            from models.branches.branch_manager import BranchManager

            branch_manager = BranchManager()
            results["components"]["branch_manager"] = "LOADED"

        except Exception as e:
            results["components"]["branch_manager"] = "ERROR"
            results["issues"].append(f"Error cargando gestor de ramas: {str(e)}")

        # Verificar modelo simple
        try:
            from modules.core.model.simple_shaili import SimpleShailiModel

            simple_model = SimpleShailiModel()
            results["components"]["simple_model"] = "LOADED"

        except Exception as e:
            results["components"]["simple_model"] = "ERROR"
            results["issues"].append(f"Error cargando modelo simple: {str(e)}")

        # Verificar base de datos
        db_path = Path("models/branch_learning.db")
        if db_path.exists():
            results["components"]["database"] = "EXISTS"
        else:
            results["components"]["database"] = "MISSING"
            results["warnings"].append("Base de datos de ramas no encontrada")

        # Determinar estado general
        if any(status == "ERROR" for status in results["components"].values()):
            results["integration_status"] = "ERROR"
        elif any(status == "MISSING" for status in results["components"].values()):
            results["integration_status"] = "WARNING"
        else:
            results["integration_status"] = "OK"

        return results

    def audit_memory_usage(self) -> Dict[str, Any]:
        """
        Auditar uso de memoria

        Returns:
            Dict[str, Any]: Resultados de la auditor√≠a
        """
        logger.info("üîç Auditar uso de memoria")

        results = {
            "memory_status": "UNKNOWN",
            "gpu_available": torch.cuda.is_available(),
            "memory_usage": {},
            "warnings": [],
        }

        if torch.cuda.is_available():
            results["memory_usage"]["gpu_total"] = round(
                torch.cuda.get_device_properties(0).total_memory / (1024**3), 2
            )
            results["memory_usage"]["gpu_allocated"] = round(
                torch.cuda.memory_allocated(0) / (1024**3), 2
            )
            results["memory_usage"]["gpu_cached"] = round(
                torch.cuda.memory_reserved(0) / (1024**3), 2
            )

            # Verificar si hay suficiente memoria para los modelos
            if results["memory_usage"]["gpu_total"] < 4:
                results["warnings"].append("GPU con poca memoria (< 4GB)")
        else:
            results["warnings"].append("GPU no disponible, usando CPU")

        return results

    def run_full_audit(self) -> Dict[str, Any]:
        """
        Ejecutar auditor√≠a completa

        Returns:
            Dict[str, Any]: Resultados completos de la auditor√≠a
        """
        logger.info("üöÄ Iniciando auditor√≠a completa del sistema")

        audit_results = {
            "timestamp": str(torch.datetime.now()),
            "main_model": self.audit_main_model(),
            "branch_model": self.audit_branch_model(),
            "system_integration": self.audit_system_integration(),
            "memory_usage": self.audit_memory_usage(),
            "summary": {},
        }

        # Generar resumen
        main_status = audit_results["main_model"]["status"]
        branch_status = audit_results["branch_model"]["status"]
        integration_status = audit_results["system_integration"]["integration_status"]

        if (
            main_status == "LOADED"
            and branch_status == "LOADED"
            and integration_status == "OK"
        ):
            audit_results["summary"]["overall_status"] = "HEALTHY"
            audit_results["summary"][
                "message"
            ] = "Todos los modelos funcionando correctamente"
        elif main_status == "LOADED" and branch_status == "LOADED":
            audit_results["summary"]["overall_status"] = "WARNING"
            audit_results["summary"][
                "message"
            ] = "Modelos cargados pero problemas de integraci√≥n"
        else:
            audit_results["summary"]["overall_status"] = "CRITICAL"
            audit_results["summary"]["message"] = "Problemas cr√≠ticos con los modelos"

        return audit_results

    def generate_report(self, audit_results: Dict[str, Any]) -> str:
        """
        Generar reporte de auditor√≠a

        Args:
            audit_results (Dict[str, Any]): Resultados de la auditor√≠a

        Returns:
            str: Reporte formateado
        """
        report = []
        report.append("=" * 80)
        report.append("REPORTE DE AUDITOR√çA - SHAILI AI")
        report.append("=" * 80)
        report.append(f"Fecha: {audit_results['timestamp']}")
        report.append("")

        # Resumen general
        summary = audit_results["summary"]
        report.append(f"ESTADO GENERAL: {summary['overall_status']}")
        report.append(f"Mensaje: {summary['message']}")
        report.append("")

        # Modelo principal
        main = audit_results["main_model"]
        report.append("üìä MODELO PRINCIPAL (Shaili Personal Model)")
        report.append(f"   Estado: {main['status']}")
        report.append(f"   Cuantizaci√≥n: {main['quantization']}")

        if main["files"]:
            report.append("   Archivos:")
            for file, info in main["files"].items():
                if info["exists"]:
                    report.append(f"     ‚úÖ {file} ({info['size_mb']} MB)")
                else:
                    report.append(f"     ‚ùå {file} (FALTANTE)")

        if main["issues"]:
            report.append("   Problemas:")
            for issue in main["issues"]:
                report.append(f"     ‚ùå {issue}")

        report.append("")

        # Modelo de ramas
        branch = audit_results["branch_model"]
        report.append("üîç MODELO DE RAMAS (paraphrase-multilingual-MiniLM-L12-v2)")
        report.append(f"   Estado: {branch['status']}")
        report.append(f"   Cuantizaci√≥n: {branch['quantization']}")

        if branch["performance"]:
            report.append("   Rendimiento:")
            for key, value in branch["performance"].items():
                report.append(f"     {key}: {value}")

        if branch["issues"]:
            report.append("   Problemas:")
            for issue in branch["issues"]:
                report.append(f"     ‚ùå {issue}")

        report.append("")

        # Integraci√≥n del sistema
        integration = audit_results["system_integration"]
        report.append("üîß INTEGRACI√ìN DEL SISTEMA")
        report.append(f"   Estado: {integration['integration_status']}")

        for component, status in integration["components"].items():
            if status == "LOADED":
                report.append(f"     ‚úÖ {component}: {status}")
            elif status == "EXISTS":
                report.append(f"     ‚úÖ {component}: {status}")
            else:
                report.append(f"     ‚ùå {component}: {status}")

        report.append("")

        # Uso de memoria
        memory = audit_results["memory_usage"]
        report.append("üíæ USO DE MEMORIA")
        report.append(f"   GPU disponible: {memory['gpu_available']}")

        if memory["gpu_available"]:
            for key, value in memory["memory_usage"].items():
                report.append(f"   {key}: {value} GB")

        if memory["warnings"]:
            report.append("   Advertencias:")
            for warning in memory["warnings"]:
                report.append(f"     ‚ö†Ô∏è {warning}")

        report.append("")
        report.append("=" * 80)

        return "\n".join(report)


def main():
    """Funci√≥n principal de auditor√≠a"""
    logger.info("üöÄ Iniciando auditor√≠a de modelos de Shaili AI")

    auditor = ModelAuditor()

    # Ejecutar auditor√≠a completa
    results = auditor.run_full_audit()

    # Generar y mostrar reporte
    report = auditor.generate_report(results)
    print(report)

    # Guardar reporte en archivo
    report_path = "audit_report.txt"
    with open(report_path, "w") as f:
        f.write(report)

    logger.info(f"üìÑ Reporte guardado en: {report_path}")

    # Retornar c√≥digo de salida seg√∫n el estado
    if results["summary"]["overall_status"] == "HEALTHY":
        logger.info("‚úÖ Auditor√≠a completada - Sistema saludable")
        return 0
    elif results["summary"]["overall_status"] == "WARNING":
        logger.warning("‚ö†Ô∏è Auditor√≠a completada - Advertencias encontradas")
        return 1
    else:
        logger.error("‚ùå Auditor√≠a completada - Problemas cr√≠ticos")
        return 2


if __name__ == "__main__":
    exit(main())
