"""
Sistema de Respaldo Inteligente para Shaili AI
==============================================

Sistema inteligente de respaldo y recuperaci√≥n de datos que proporciona
respuestas de respaldo cuando el sistema principal no est√° disponible.
"""

import logging
import json
import os
import time
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np

# Agregar path para importar m√≥dulos
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "../.."))

try:
    from modules.core.model.simple_shaili import SimpleShailiModel
    from modules.ai.text_processor import TextProcessor
    from modules.ai.semantic_analyzer import SemanticAnalyzer

    LLM_AVAILABLE = True
except ImportError as e:
    logging.error(f"Error importando m√≥dulos: {e}")
    LLM_AVAILABLE = False

logger = logging.getLogger(__name__)


class IntelligentBackupSystem:
    """
    Sistema inteligente de respaldo que proporciona respuestas de respaldo
    cuando el sistema principal falla o no est√° disponible
    """

    def __init__(self, backup_data_path: str = "data/backup/"):
        self.logger = logging.getLogger(__name__)
        self.backup_data_path = backup_data_path
        self.knowledge_base = {}
        self.response_patterns = {}
        self.query_history = []
        self.backup_metrics = {
            "total_queries": 0,
            "successful_responses": 0,
            "failed_responses": 0,
            "average_confidence": 0.0,
        }

        self._initialize_backup_system()

    def _initialize_backup_system(self):
        """Inicializar sistema de respaldo con datos reales"""
        try:
            # Crear directorio de respaldo si no existe
            os.makedirs(self.backup_data_path, exist_ok=True)

            # Cargar base de conocimiento de respaldo desde archivos reales
            self._load_backup_knowledge_base()

            # Cargar patrones de respuesta desde archivos reales
            self._load_response_patterns()

            self.logger.info("‚úÖ Sistema de respaldo inteligente inicializado")

        except Exception as e:
            self.logger.error(f"‚ùå Error inicializando sistema de respaldo: {e}")

    def _load_backup_knowledge_base(self):
        """Cargar base de conocimiento de respaldo desde archivos reales"""
        try:
            knowledge_file = os.path.join(self.backup_data_path, "knowledge_base.json")

            if os.path.exists(knowledge_file):
                with open(knowledge_file, "r", encoding="utf-8") as f:
                    self.knowledge_base = json.load(f)
                self.logger.info(
                    f"Base de conocimiento cargada: {len(self.knowledge_base)} entradas"
                )
            else:
                # Crear base de conocimiento b√°sica si no existe
                self.knowledge_base = self._create_basic_knowledge_base()
                self._save_knowledge_base()

        except Exception as e:
            self.logger.error(f"Error cargando base de conocimiento: {e}")
            self.knowledge_base = self._create_basic_knowledge_base()

    def _create_basic_knowledge_base(self) -> Dict[str, Any]:
        """Crear base de conocimiento b√°sica real"""
        return {
            "greetings": {
                "patterns": ["hola", "buenos d√≠as", "buenas tardes", "saludos"],
                "responses": [
                    "¬°Hola! Soy Sheily AI. ¬øEn qu√© puedo ayudarte?",
                    "¬°Buenos d√≠as! Estoy aqu√≠ para asistirte.",
                    "¬°Hola! ¬øC√≥mo puedo ayudarte hoy?",
                ],
            },
            "model_info": {
                "patterns": ["modelo", "capacidades", "qu√© puedes hacer"],
                "responses": [
                    "Soy Sheily AI, un sistema de IA avanzado con capacidades de chat y generaci√≥n de archivos LoRA.",
                    "Puedo ayudarte con consultas, generar respuestas y crear archivos LoRA para entrenamiento.",
                    "Mis capacidades incluyen procesamiento de lenguaje natural y asistencia inteligente.",
                ],
            },
            "training": {
                "patterns": ["entrenamiento", "lora", "ramas especializadas"],
                "responses": [
                    "El sistema incluye 32 ramas especializadas que se entrenan usando archivos LoRA.",
                    "Puedo generar archivos LoRA para entrenar modelos especializados en diferentes dominios.",
                    "El entrenamiento se realiza usando el modelo de 16-bit con adaptadores LoRA.",
                ],
            },
        }

    def _save_knowledge_base(self):
        """Guardar base de conocimiento en un archivo JSON"""
        try:
            knowledge_file = os.path.join(self.backup_data_path, "knowledge_base.json")
            with open(knowledge_file, "w", encoding="utf-8") as f:
                json.dump(self.knowledge_base, f, indent=4, ensure_ascii=False)
            self.logger.info(
                f"Base de conocimiento guardada: {len(self.knowledge_base)} entradas"
            )
        except Exception as e:
            self.logger.error(f"Error guardando base de conocimiento: {e}")

    def _load_response_patterns(self):
        """Cargar patrones de respuesta desde archivos reales"""
        try:
            patterns_file = os.path.join(
                self.backup_data_path, "response_patterns.json"
            )

            if os.path.exists(patterns_file):
                with open(patterns_file, "r", encoding="utf-8") as f:
                    self.response_patterns = json.load(f)
                self.logger.info(
                    f"Patrones de respuesta cargados: {len(self.response_patterns)} patrones"
                )
            else:
                # Crear patrones de respuesta b√°sicos si no existen
                self.response_patterns = self._create_basic_response_patterns()
                self._save_response_patterns()

        except Exception as e:
            self.logger.error(f"Error cargando patrones de respuesta: {e}")
            self.response_patterns = self._create_basic_response_patterns()

    def _create_basic_response_patterns(self) -> Dict[str, Any]:
        """Crear patrones de respuesta b√°sicos reales"""
        return {
            "error_general": {
                "patterns": ["error", "problema", "fallo", "problemas"],
                "responses": [
                    "Lo siento, hubo un error en el procesamiento. D√©jame intentar ayudarte de otra manera.",
                    "Parece que hay un problema t√©cnico. Te ayudo con una respuesta alternativa.",
                    "El sistema principal no est√° disponible. Te proporciono asistencia b√°sica.",
                ],
            },
            "unavailable_service": {
                "patterns": ["servicio", "disponible", "ocupado", "no disponible"],
                "responses": [
                    "El servicio no est√° disponible en este momento. ¬øPuedo ayudarte con algo m√°s?",
                    "Hay un problema temporal. Te ayudo con informaci√≥n b√°sica.",
                    "El sistema est√° ocupado. Te respondo con lo que puedo.",
                ],
            },
        }

    def _save_response_patterns(self):
        """Guardar patrones de respuesta en un archivo JSON"""
        try:
            patterns_file = os.path.join(
                self.backup_data_path, "response_patterns.json"
            )
            with open(patterns_file, "w", encoding="utf-8") as f:
                json.dump(self.response_patterns, f, indent=4, ensure_ascii=False)
            self.logger.info(
                f"Patrones de respuesta guardados: {len(self.response_patterns)} patrones"
            )
        except Exception as e:
            self.logger.error(f"Error guardando patrones de respuesta: {e}")

    def generate_backup_response(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
        error_type: str = "general",
    ) -> Dict[str, Any]:
        """
        Genera una respuesta de respaldo usando el modelo principal real

        Args:
            query: Consulta del usuario
            context: Contexto adicional
            error_type: Tipo de error que activ√≥ el respaldo

        Returns:
            Dict con la respuesta y metadatos
        """
        try:
            start_time = datetime.now()

            # Intentar con el modelo principal real
            if self.llm_model:
                # Generar respuesta con el modelo real
                response = self.llm_model.generate_text(
                    prompt=query, max_length=512, temperature=0.7, top_p=0.9
                )

                # Procesar respuesta
                if self.text_processor:
                    text_analysis = self.text_processor.analyze_text(response)
                    word_count = text_analysis.word_count
                else:
                    word_count = len(response.split())

                processing_time = (datetime.now() - start_time).total_seconds()

                return {
                    "success": True,
                    "response": response,
                    "source": "main_model_backup",
                    "error_type": error_type,
                    "processing_time": processing_time,
                    "word_count": word_count,
                    "confidence": 0.8,
                    "metadata": {
                        "backup_used": True,
                        "model_used": "shaili-personal-model",
                        "quantization": "4bit",
                    },
                }

            # Si no hay modelo, usar respuestas predefinidas
            else:
                import random

                backup_response = random.choice(
                    self.backup_responses.get(
                        error_type, self.backup_responses["error"]
                    )
                )

                return {
                    "success": True,
                    "response": f"{backup_response} Tu consulta fue: '{query}'. Te ayudo cuando el sistema est√© disponible.",
                    "source": "predefined_backup",
                    "error_type": error_type,
                    "processing_time": 0.1,
                    "word_count": len(query.split()),
                    "confidence": 0.3,
                    "metadata": {
                        "backup_used": True,
                        "model_used": "predefined",
                        "quantization": "none",
                    },
                }

        except Exception as e:
            logger.error(f"‚ùå Error en sistema de respaldo: {e}")

            # Respuesta de emergencia
            return {
                "success": False,
                "response": "Error cr√≠tico en el sistema de respaldo. Por favor, intenta m√°s tarde.",
                "source": "emergency",
                "error_type": error_type,
                "processing_time": 0.0,
                "word_count": 0,
                "confidence": 0.0,
                "metadata": {
                    "backup_used": True,
                    "model_used": "emergency",
                    "error": str(e),
                },
            }

    def analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """Analizar la intenci√≥n de la consulta usando componentes reales"""
        try:
            if self.semantic_analyzer:
                # An√°lisis sem√°ntico real
                analysis = self.semantic_analyzer.analyze_text(query)
                return {
                    "intent": analysis.get("intent", "general"),
                    "confidence": analysis.get("confidence", 0.5),
                    "entities": analysis.get("entities", []),
                    "sentiment": analysis.get("sentiment", "neutral"),
                }
            else:
                # An√°lisis b√°sico
                query_lower = query.lower()
                if "?" in query:
                    intent = "question"
                elif any(
                    word in query_lower for word in ["ayuda", "ayudar", "asistir"]
                ):
                    intent = "help"
                else:
                    intent = "statement"

                return {
                    "intent": intent,
                    "confidence": 0.6,
                    "entities": [],
                    "sentiment": "neutral",
                }

        except Exception as e:
            logger.error(f"Error analizando intenci√≥n: {e}")
            return {
                "intent": "unknown",
                "confidence": 0.0,
                "entities": [],
                "sentiment": "neutral",
            }

    def get_system_status(self) -> Dict[str, Any]:
        """Obtener estado del sistema de respaldo"""
        return {
            "backup_system_available": LLM_AVAILABLE,
            "main_model_loaded": self.llm_model is not None,
            "text_processor_loaded": self.text_processor is not None,
            "semantic_analyzer_loaded": self.semantic_analyzer is not None,
            "timestamp": datetime.now().isoformat(),
        }


def main():
    """Funci√≥n principal para pruebas"""
    print("üß† SISTEMA DE RESPALDO INTELIGENTE - PRUEBAS")
    print("=" * 50)

    system = IntelligentBackupSystem()

    # Probar estado del sistema
    status = system.get_system_status()
    print(f"Estado del sistema: {status}")

    # Probar respuesta de respaldo
    test_query = "¬øQu√© es la inteligencia artificial?"
    result = system.generate_backup_response(test_query, error_type="general")

    print(f"\nConsulta: {test_query}")
    print(f"Respuesta: {result['response']}")
    print(f"Fuente: {result['source']}")
    print(f"Confianza: {result['confidence']}")
    print(f"Tiempo: {result['processing_time']}s")

    # Probar an√°lisis de intenci√≥n
    intent_analysis = system.analyze_query_intent(test_query)
    print(f"\nAn√°lisis de intenci√≥n: {intent_analysis}")


if __name__ == "__main__":
    main()
