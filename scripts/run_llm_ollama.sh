#!/usr/bin/env bash
set -euo pipefail

# Script para levantar y configurar Ollama con modelo SHEILY
# Autor: SHEILY AI System
# Fecha: $(date +%Y-%m-%d)

echo "ğŸš€ Iniciando servicio LLM con Ollama..."

# Verificar que Docker estÃ© disponible
if ! command -v docker &> /dev/null; then
    echo "âŒ Error: Docker no estÃ¡ instalado o no estÃ¡ en el PATH"
    echo "   Por favor instala Docker antes de continuar"
    exit 1
fi

if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
    echo "âŒ Error: docker-compose no estÃ¡ disponible"
    echo "   Por favor instala docker-compose o usa Docker con soporte para 'compose'"
    exit 1
fi

# Navegar al directorio del proyecto
cd "$(dirname "$0")/.."

echo "ğŸ“ Directorio de trabajo: $(pwd)"

# Levantar el servicio Ollama
echo "ğŸ³ Levantando contenedor Ollama..."
if command -v docker-compose &> /dev/null; then
    docker-compose -f llm/docker-compose.llm.yml up -d
else
    docker compose -f llm/docker-compose.llm.yml up -d
fi

# Esperar a que el servicio estÃ© disponible
echo "â³ Esperando a que Ollama estÃ© listo..."
max_attempts=60
attempt=0

while [ $attempt -lt $max_attempts ]; do
    if curl -sSf http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "âœ… Ollama estÃ¡ listo!"
        break
    fi
    
    echo "   Intento $((attempt + 1))/$max_attempts - Esperando..."
    sleep 2
    attempt=$((attempt + 1))
done

if [ $attempt -eq $max_attempts ]; then
    echo "âŒ Error: Ollama no respondiÃ³ despuÃ©s de $((max_attempts * 2)) segundos"
    echo "   Revisa los logs con: docker logs sheily-llm-ollama"
    exit 1
fi

# Descargar modelo base si no existe
echo "ğŸ“¥ Verificando modelo base llama3.2:3b-instruct..."
if ! docker exec sheily-llm-ollama ollama list | grep -q "llama3.2:3b-instruct"; then
    echo "ğŸ“¥ Descargando modelo base llama3.2:3b-instruct..."
    docker exec sheily-llm-ollama ollama pull llama3.2:3b-instruct
    echo "âœ… Modelo base descargado"
else
    echo "âœ… Modelo base ya disponible"
fi

# Crear modelo personalizado SHEILY
echo "ğŸ”§ Creando modelo personalizado SHEILY..."
docker exec -w /workspace sheily-llm-ollama ollama create sheily-llm -f Modelfile

# Verificar que el modelo se creÃ³ correctamente
if docker exec sheily-llm-ollama ollama list | grep -q "sheily-llm"; then
    echo "âœ… Modelo SHEILY creado exitosamente"
else
    echo "âŒ Error: No se pudo crear el modelo SHEILY"
    exit 1
fi

# Probar el modelo con una consulta simple
echo "ğŸ§ª Probando el modelo SHEILY..."
test_response=$(curl -s http://localhost:11434/api/generate \
  -d '{"model":"sheily-llm","prompt":"Hola, Â¿cÃ³mo estÃ¡s?","stream":false}' \
  | jq -r '.response' 2>/dev/null || echo "Error en respuesta")

if [ "$test_response" != "Error en respuesta" ] && [ -n "$test_response" ]; then
    echo "âœ… Modelo SHEILY funcionando correctamente"
    echo "ğŸ“ Respuesta de prueba: ${test_response:0:100}..."
else
    echo "âš ï¸  Advertencia: No se pudo verificar la respuesta del modelo"
fi

echo ""
echo "ğŸ‰ Â¡Sistema LLM SHEILY listo!"
echo "ğŸ“ Endpoint: http://localhost:11434"
echo "ğŸ¤– Modelo: sheily-llm"
echo ""
echo "ğŸ“‹ Comandos Ãºtiles:"
echo "   Ver modelos: docker exec sheily-llm-ollama ollama list"
echo "   Ver logs: docker logs sheily-llm-ollama"
echo "   Parar servicio: docker-compose -f llm/docker-compose.llm.yml down"
echo ""
echo "ğŸ§ª Probar con curl:"
echo '   curl http://localhost:11434/api/generate -d '"'"'{"model":"sheily-llm","prompt":"Dame un plan de test de seguridad en 5 pasos"}'"'"
