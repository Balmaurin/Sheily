#!/usr/bin/env python3
"""
Script de prueba para verificar la integraciÃ³n del sistema LLM
"""

import sys
import os
import time
import requests
import json
from datetime import datetime

# AÃ±adir el directorio backend al path
sys.path.append(os.path.join(os.path.dirname(__file__), "..", "backend"))


def test_ollama_service():
    """Probar que el servicio Ollama estÃ© funcionando"""
    print("ğŸ” Probando servicio Ollama...")

    try:
        # Verificar que el servicio estÃ© disponible
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        response.raise_for_status()

        models = response.json().get("models", [])
        model_names = [model["name"] for model in models]

        print(f"âœ… Servicio Ollama disponible")
        print(f"ğŸ“‹ Modelos disponibles: {model_names}")

        # Verificar que el modelo sheily-llm estÃ© disponible
        if "sheily-llm" in model_names:
            print("âœ… Modelo sheily-llm disponible")
            return True
        else:
            print("âŒ Modelo sheily-llm no encontrado")
            return False

    except requests.exceptions.RequestException as e:
        print(f"âŒ Error conectando con Ollama: {e}")
        return False


def test_llm_client():
    """Probar el cliente LLM"""
    print("\nğŸ” Probando cliente LLM...")

    try:
        from llm_client import get_llm_client

        # Obtener cliente
        client = get_llm_client()
        print(f"âœ… Cliente LLM inicializado - Modo: {client.llm_mode}")

        # Verificar salud
        health = client.health_check()
        print(f"ğŸ“Š Estado del servicio: {health['status']}")

        if health["status"] == "healthy":
            # Probar chat simple
            messages = [{"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}]
            response = client.llm_chat(messages, max_tokens=50)
            print(f"âœ… Chat funcionando - Respuesta: {response[:100]}...")

            return True
        else:
            print(f"âŒ Servicio no saludable: {health}")
            return False

    except Exception as e:
        print(f"âŒ Error en cliente LLM: {e}")
        return False


def test_orchestrator_integration():
    """Probar integraciÃ³n con el orquestador"""
    print("\nğŸ” Probando integraciÃ³n con orquestador...")

    try:
        # AÃ±adir path para mÃ³dulos
        sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

        from modules.orchestrator.main_orchestrator import MainOrchestrator

        # Inicializar orquestador
        orchestrator = MainOrchestrator()
        print("âœ… Orquestador inicializado")

        # Verificar estado del LLM
        llm_status = orchestrator.get_llm_status()
        print(f"ğŸ“Š Estado LLM en orquestador: {llm_status['status']}")

        if llm_status["status"] == "healthy":
            # Probar consulta simple
            response = orchestrator.process_query("Â¿QuÃ© es la inteligencia artificial?")
            print(
                f"âœ… Consulta procesada - Fuente: {response.get('source', 'unknown')}"
            )
            print(f"ğŸ“ Respuesta: {response.get('text', '')[:100]}...")

            return True
        else:
            print(f"âŒ LLM no disponible en orquestador: {llm_status}")
            return False

    except Exception as e:
        print(f"âŒ Error en integraciÃ³n orquestador: {e}")
        return False


def test_pipeline_enhanced():
    """Probar pipeline mejorado draft â†’ critic â†’ fix"""
    print("\nğŸ” Probando pipeline mejorado...")

    try:
        from llm_client import get_llm_client

        client = get_llm_client()

        # Probar pipeline completo
        result = client.process_pipeline(
            query="Dame un plan de seguridad informÃ¡tica bÃ¡sico",
            context="Para una pequeÃ±a empresa",
        )

        print("âœ… Pipeline completado")
        print(f"â±ï¸ Tiempo de procesamiento: {result['processing_time']:.2f}s")
        print(f"ğŸ“ Borrador: {result['draft'][:100]}...")
        print(f"ğŸ” CrÃ­tica: {result['critique'][:100]}...")
        print(f"âœ¨ Respuesta final: {result['final_response'][:100]}...")

        return True

    except Exception as e:
        print(f"âŒ Error en pipeline mejorado: {e}")
        return False


def main():
    """FunciÃ³n principal de pruebas"""
    print("ğŸš€ Iniciando pruebas de integraciÃ³n LLM SHEILY")
    print("=" * 50)

    tests = [
        ("Servicio Ollama", test_ollama_service),
        ("Cliente LLM", test_llm_client),
        ("IntegraciÃ³n Orquestador", test_orchestrator_integration),
        ("Pipeline Mejorado", test_pipeline_enhanced),
    ]

    results = []

    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ Error inesperado en {test_name}: {e}")
            results.append((test_name, False))

    # Resumen de resultados
    print("\n" + "=" * 50)
    print("ğŸ“Š RESUMEN DE PRUEBAS")
    print("=" * 50)

    passed = 0
    total = len(results)

    for test_name, result in results:
        status = "âœ… PASÃ“" if result else "âŒ FALLÃ“"
        print(f"{test_name:.<30} {status}")
        if result:
            passed += 1

    print(f"\nResultado: {passed}/{total} pruebas pasaron")

    if passed == total:
        print("ğŸ‰ Â¡Todas las pruebas pasaron! Sistema LLM listo.")
        return 0
    else:
        print("âš ï¸ Algunas pruebas fallaron. Revisar configuraciÃ³n.")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
