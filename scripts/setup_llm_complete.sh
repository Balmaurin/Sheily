#!/usr/bin/env bash
set -euo pipefail

# Script completo de configuraci√≥n del sistema LLM SHEILY
# Autor: SHEILY AI System
# Fecha: $(date +%Y-%m-%d)

echo "üöÄ Configuraci√≥n completa del sistema LLM SHEILY"
echo "================================================"

# Verificar prerrequisitos
echo "üîç Verificando prerrequisitos..."

# Verificar Docker
if ! command -v docker &> /dev/null; then
    echo "‚ùå Error: Docker no est√° instalado"
    echo "   Instala Docker antes de continuar:"
    echo "   sudo apt update && sudo apt install docker.io docker-compose"
    exit 1
fi

# Verificar docker-compose
if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
    echo "‚ùå Error: docker-compose no est√° disponible"
    echo "   Instala docker-compose antes de continuar"
    exit 1
fi

# Verificar Python
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Error: Python 3 no est√° instalado"
    exit 1
fi

echo "‚úÖ Prerrequisitos verificados"

# Navegar al directorio del proyecto
cd "$(dirname "$0")/.."
echo "üìÅ Directorio de trabajo: $(pwd)"

# Paso 1: Levantar servicio LLM
echo ""
echo "üê≥ Paso 1: Levantando servicio LLM con Ollama..."
./scripts/run_llm_ollama.sh

if [ $? -ne 0 ]; then
    echo "‚ùå Error levantando servicio LLM"
    exit 1
fi

# Paso 2: Verificar instalaci√≥n
echo ""
echo "üß™ Paso 2: Verificando instalaci√≥n..."

# Esperar un momento para que el servicio se estabilice
sleep 5

# Verificar que el servicio est√© respondiendo
if ! curl -sSf http://localhost:11434/api/tags >/dev/null; then
    echo "‚ùå Error: Servicio Ollama no responde"
    echo "   Revisa los logs: docker logs sheily-llm-ollama"
    exit 1
fi

echo "‚úÖ Servicio LLM funcionando"

# Paso 3: Probar integraci√≥n
echo ""
echo "üîß Paso 3: Probando integraci√≥n del sistema..."

# Ejecutar pruebas de integraci√≥n
python3 scripts/test_llm_integration.py

if [ $? -eq 0 ]; then
    echo "‚úÖ Todas las pruebas pasaron"
else
    echo "‚ö†Ô∏è Algunas pruebas fallaron, pero el sistema b√°sico est√° funcionando"
fi

# Paso 4: Mostrar informaci√≥n de uso
echo ""
echo "üéâ ¬°Sistema LLM SHEILY configurado exitosamente!"
echo "================================================"
echo ""
echo "üìç Endpoints disponibles:"
echo "   ‚Ä¢ Ollama API: http://localhost:11434"
echo "   ‚Ä¢ Modelo SHEILY: sheily-llm"
echo ""
echo "üîß Comandos √∫tiles:"
echo "   ‚Ä¢ Ver modelos: docker exec sheily-llm-ollama ollama list"
echo "   ‚Ä¢ Ver logs: docker logs sheily-llm-ollama"
echo "   ‚Ä¢ Parar servicio: docker-compose -f llm/docker-compose.llm.yml down"
echo "   ‚Ä¢ Reiniciar: docker-compose -f llm/docker-compose.llm.yml restart"
echo ""
echo "üß™ Probar el sistema:"
echo '   curl http://localhost:11434/api/generate -d '"'"'{"model":"sheily-llm","prompt":"Hola SHEILY, ¬øc√≥mo est√°s?"}'"'"
echo ""
echo "üìö Documentaci√≥n:"
echo "   ‚Ä¢ README_LLM.md - Gu√≠a completa del sistema"
echo "   ‚Ä¢ docs/ORCHESTRATOR_GUIDE.md - Gu√≠a del orquestador"
echo ""
echo "üöÄ ¬°El sistema est√° listo para usar!"

# Mostrar estado final
echo ""
echo "üìä Estado del sistema:"
docker ps | grep ollama || echo "   ‚ö†Ô∏è Contenedor Ollama no encontrado"
curl -s http://localhost:11434/api/tags | jq -r '.models[].name' 2>/dev/null || echo "   ‚ö†Ô∏è No se pudieron listar los modelos"
