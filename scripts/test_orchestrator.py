#!/usr/bin/env python3
"""
Script de Prueba del Orquestador
================================

Prueba completa del sistema de orquestaciÃ³n
"""

import os
import sys
import json
import time
import requests
from datetime import datetime

# Agregar el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def test_orchestrator_direct():
    """Probar orquestador directamente"""
    print("ðŸ§ª Probando orquestador directamente...")

    try:
        from modules.orchestrator.main_orchestrator import MainOrchestrator

        # Crear orquestador
        orchestrator = MainOrchestrator()
        print("âœ… Orquestador creado")

        # Consultas de prueba
        test_queries = [
            "Â¿QuÃ© es la inteligencia artificial?",
            "Explica el teorema de PitÃ¡goras",
            "Â¿CÃ³mo funciona la fotosÃ­ntesis?",
            "CuÃ©ntame sobre la historia de Roma",
            "Â¿CuÃ¡l es la capital de Francia?",
        ]

        results = []

        for i, query in enumerate(test_queries, 1):
            print(f"\nðŸ“ Consulta {i}: {query}")

            start_time = time.time()
            response = orchestrator.process_query(query)
            response_time = time.time() - start_time

            print(f"â±ï¸  Tiempo de respuesta: {response_time:.2f}s")
            print(f"ðŸ·ï¸  Dominio: {response.get('domain', 'N/A')}")
            print(f"ðŸ›£ï¸  Ruta: {response.get('route_type', 'N/A')}")
            print(f"ðŸ“„ Respuesta: {response.get('text', 'N/A')[:100]}...")

            results.append(
                {
                    "query": query,
                    "response_time": response_time,
                    "domain": response.get("domain"),
                    "route_type": response.get("route_type"),
                    "success": "text" in response,
                }
            )

        # Mostrar estadÃ­sticas
        successful = sum(1 for r in results if r["success"])
        avg_time = sum(r["response_time"] for r in results) / len(results)

        print(f"\nðŸ“Š EstadÃ­sticas:")
        print(f"   âœ… Exitosas: {successful}/{len(results)}")
        print(f"   â±ï¸  Tiempo promedio: {avg_time:.2f}s")

        return successful == len(results)

    except Exception as e:
        print(f"âŒ Error en prueba directa: {e}")
        return False


def test_orchestrator_api():
    """Probar orquestador a travÃ©s de API"""
    print("\nðŸŒ Probando orquestador a travÃ©s de API...")

    base_url = "http://127.0.0.1:8000/api/orchestrator"

    try:
        # Probar health check
        print("ðŸ” Health check...")
        response = requests.get(f"{base_url}/health", timeout=10)

        if response.status_code == 200:
            print("âœ… Health check exitoso")
            health_data = response.json()
            print(
                f"ðŸ“Š Estado: {health_data.get('data', {}).get('orchestrator_status')}"
            )
        else:
            print(f"âŒ Health check fallÃ³: {response.status_code}")
            return False

        # Probar procesamiento de consulta
        print("\nðŸ“ Procesando consulta...")
        test_query = {
            "query": "Â¿QuÃ© es la inteligencia artificial?",
            "user_context": {"language": "es"},
        }

        response = requests.post(f"{base_url}/query", json=test_query, timeout=30)

        if response.status_code == 200:
            print("âœ… Consulta procesada exitosamente")
            query_data = response.json()
            print(
                f"ðŸ“„ Respuesta: {query_data.get('data', {}).get('text', '')[:100]}..."
            )
        else:
            print(f"âŒ Error procesando consulta: {response.status_code}")
            return False

        # Probar estado del sistema
        print("\nðŸ“Š Estado del sistema...")
        response = requests.get(f"{base_url}/status", timeout=10)

        if response.status_code == 200:
            print("âœ… Estado del sistema obtenido")
            status_data = response.json()
            metrics = status_data.get("data", {}).get("metrics", {})
            print(f"ðŸ“ˆ Total requests: {metrics.get('total_requests', 0)}")
        else:
            print(f"âŒ Error obteniendo estado: {response.status_code}")
            return False

        # Probar mÃ©tricas
        print("\nðŸ“ˆ MÃ©tricas...")
        response = requests.get(f"{base_url}/metrics", timeout=10)

        if response.status_code == 200:
            print("âœ… MÃ©tricas obtenidas")
            metrics_data = response.json()
            performance = metrics_data.get("data", {}).get("performance", {})
            print(f"ðŸŽ¯ Tasa de Ã©xito: {performance.get('success_rate', 0):.2%}")
        else:
            print(f"âŒ Error obteniendo mÃ©tricas: {response.status_code}")
            return False

        return True

    except requests.exceptions.ConnectionError:
        print("âŒ No se pudo conectar al servidor API")
        print(
            "ðŸ’¡ AsegÃºrate de que el servidor backend estÃ© ejecutÃ¡ndose en puerto 8000"
        )
        return False
    except Exception as e:
        print(f"âŒ Error en prueba de API: {e}")
        return False


def test_branch_management():
    """Probar gestiÃ³n de ramas"""
    print("\nðŸŒ¿ Probando gestiÃ³n de ramas...")

    try:
        from branches.branch_manager import BranchManager

        # Crear gestor de ramas
        branch_manager = BranchManager()
        print("âœ… Gestor de ramas creado")

        # Obtener dominios disponibles
        domains = branch_manager.get_available_domains()
        print(f"ðŸ“š Dominios disponibles: {len(domains)}")

        # Mostrar algunos dominios
        for i, domain in enumerate(domains[:5], 1):
            print(f"   {i}. {domain}")

        if len(domains) > 5:
            print(f"   ... y {len(domains) - 5} mÃ¡s")

        # Probar estado de una rama especÃ­fica
        if domains:
            test_domain = domains[0]
            status = branch_manager.get_branch_status(test_domain)
            print(f"\nðŸ“Š Estado de '{test_domain}':")
            print(f"   ðŸ·ï¸  Nombre: {status.get('branch_name')}")
            print(f"   ðŸ”§ Adapter existe: {status.get('adapter_exists')}")
            print(f"   ðŸŒ± Micro-ramas: {len(status.get('micro_branches', []))}")

        return True

    except Exception as e:
        print(f"âŒ Error probando gestiÃ³n de ramas: {e}")
        return False


def test_adapter_policy():
    """Probar polÃ­tica de adapters"""
    print("\nâš™ï¸ Probando polÃ­tica de adapters...")

    try:
        from branches.adapter_policy import AdapterUpdatePolicy

        # Crear polÃ­tica de adapters
        policy = AdapterUpdatePolicy()
        print("âœ… PolÃ­tica de adapters creada")

        # Probar gestiÃ³n de adapters
        test_metrics = {"accuracy": 0.85, "response_time": 1.2, "memory_usage": 0.6}

        result = policy.manage_domain_adapters("MatemÃ¡ticas", test_metrics)
        print("âœ… GestiÃ³n de adapters probada")

        # Probar optimizaciÃ³n de cachÃ©
        optimization = policy.optimize_cache()
        print("âœ… OptimizaciÃ³n de cachÃ© probada")

        # Obtener mÃ©tricas de rendimiento
        metrics = policy.get_all_performance_metrics()
        print(f"ðŸ“Š MÃ©tricas de rendimiento: {len(metrics)} dominios")

        return True

    except Exception as e:
        print(f"âŒ Error probando polÃ­tica de adapters: {e}")
        return False


def generate_test_report(results):
    """Generar reporte de pruebas"""
    print("\nðŸ“Š Generando reporte de pruebas...")

    report = {
        "timestamp": datetime.now().isoformat(),
        "tests": results,
        "summary": {
            "total_tests": len(results),
            "passed_tests": sum(results.values()),
            "success_rate": sum(results.values()) / len(results) if results else 0,
        },
    }

    # Guardar reporte
    report_path = "logs/orchestrator_test_report.json"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)

    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"ðŸ“„ Reporte guardado en: {report_path}")

    return report


def main():
    """FunciÃ³n principal"""
    print("ðŸ§ª Pruebas del Orquestador de Shaili-AI")
    print("=" * 50)

    results = {}

    # Ejecutar pruebas
    tests = [
        ("Orquestador Directo", test_orchestrator_direct),
        ("GestiÃ³n de Ramas", test_branch_management),
        ("PolÃ­tica de Adapters", test_adapter_policy),
        ("API del Orquestador", test_orchestrator_api),
    ]

    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            results[test_name] = test_func()
            status = "âœ… PASÃ“" if results[test_name] else "âŒ FALLÃ“"
            print(f"\n{status} - {test_name}")
        except Exception as e:
            print(f"âŒ ERROR - {test_name}: {e}")
            results[test_name] = False

    # Generar reporte
    report = generate_test_report(results)

    # Mostrar resumen
    print(f"\n{'='*50}")
    print("ðŸ“Š RESUMEN DE PRUEBAS")
    print(f"{'='*50}")

    for test_name, passed in results.items():
        status = "âœ… PASÃ“" if passed else "âŒ FALLÃ“"
        print(f"{status} - {test_name}")

    summary = report["summary"]
    print(
        f"\nðŸŽ¯ Total: {summary['passed_tests']}/{summary['total_tests']} pruebas exitosas"
    )
    print(f"ðŸ“ˆ Tasa de Ã©xito: {summary['success_rate']:.1%}")

    if summary["success_rate"] >= 0.75:
        print("\nðŸŽ‰ Â¡El orquestador estÃ¡ funcionando correctamente!")
    else:
        print("\nâš ï¸  Algunas pruebas fallaron. Revisa los logs para mÃ¡s detalles.")

    return summary["success_rate"] >= 0.75


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
