# Estado del Servidor LLM Local

## ‚úÖ Componentes operativos

| Componente | Estado | Descripci√≥n |
|------------|--------|-------------|
| Servidor Flask (`backend/llm_server.py`) | ‚úÖ | Expone `/v1/chat/completions`, `/chat`, `/health` y `/v1/models`.
| Modelo Llama‚Äë3.2‚Äë3B‚ÄëInstruct‚ÄëQ8_0 | ‚úÖ | Archivo GGUF cargado mediante `llama_cpp`.
| Cliente Python (`backend/llm_client.py`) | ‚úÖ | Pipeline draft ‚Üí critic ‚Üí fix contra el servidor local.
| Integraci√≥n backend Node | ‚úÖ | Servicios Express consumen exclusivamente el endpoint local.

## üîÅ Flujo actual

1. El servidor se inicia con `python backend/llm_server.py` y carga el modelo desde `LLM_MODEL_PATH`.
2. El backend llama a `/v1/chat/completions` para cada mensaje del dashboard.
3. Las m√©tricas de uso y tiempo de inferencia se devuelven directamente al frontend; no hay datos simulados.

## ‚ö†Ô∏è Advertencias

- Si el archivo GGUF no est√° disponible, `/health` responder√° con `status: error` y el backend propagar√° la incidencia al usuario.
- No existe infraestructura de respaldo: cualquier interrupci√≥n del servidor detendr√° el chat hasta que el modelo vuelva a cargarse.

## ‚úÖ Pr√≥ximos pasos recomendados

- Automatizar la verificaci√≥n de disponibilidad del archivo GGUF antes de iniciar el backend.
- A√±adir scripts de supervisi√≥n que reinicien el servidor en caso de fallo.

El entorno ya no depende de Ollama ni de servicios de Hugging Face. Toda la inferencia se resuelve localmente.

