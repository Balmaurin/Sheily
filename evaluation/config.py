"""
Configuraci√≥n del Sistema de Evaluaci√≥n - Shaili AI

Este archivo contiene todas las configuraciones necesarias para el sistema
de evaluaci√≥n de calidad de respuestas de IA.
"""

import os
from typing import Dict, Any


class EvaluationConfig:
    """Configuraci√≥n centralizada para el sistema de evaluaci√≥n"""

    # Configuraci√≥n de directorios
    LOG_DIR = "logs/evaluation"
    RESULTS_DIR = "results/evaluation"
    MODELS_DIR = "models/evaluation"

    # Configuraci√≥n de modelos de NLP
    MAIN_MODEL = "models/custom/shaili-personal-model"

    # Configuraci√≥n de umbrales de calidad
    QUALITY_THRESHOLDS = {
        "coherence": 0.6,
        "diversity": 0.5,
        "toxicity": 0.3,
        "overall": 0.7,
    }

    # Pesos para puntuaci√≥n compuesta
    QUALITY_WEIGHTS = {"coherence": 0.4, "diversity": 0.3, "toxicity": 0.3}

    # Configuraci√≥n de logging
    LOGGING_CONFIG = {
        "level": "INFO",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        "file_rotation": "daily",
        "max_files": 30,
    }

    # Configuraci√≥n de benchmark
    BENCHMARK_CONFIG = {
        "test_data_size": 500,
        "timeout_seconds": 300,
        "memory_limit_mb": 2048,
        "save_visualizations": True,
    }

    # Configuraci√≥n de toxicidad
    TOXICITY_CONFIG = {
        "categories": {
            "insultos": {
                "severity": 0.7,
                "keywords": ["idiota", "est√∫pido", "imb√©cil", "tonto", "pendejo"],
            },
            "discriminacion": {
                "severity": 0.8,
                "keywords": ["negro", "indio", "sudaca", "extranjero", "inmigrante"],
            },
            "sexismo": {
                "severity": 0.6,
                "keywords": ["machista", "feminazi", "sexista", "d√©bil", "hist√©rica"],
            },
            "violencia": {
                "severity": 0.9,
                "keywords": ["matar", "golpear", "agredir", "violencia", "muerte"],
            },
        },
        "context_penalty": 0.3,
        "max_toxicity_score": 1.0,
    }

    # Configuraci√≥n de diversidad
    DIVERSITY_CONFIG = {
        "min_sentence_length": 5,
        "max_sentence_length": 50,
        "min_lexical_diversity": 0.3,
        "min_syntactic_complexity": 0.4,
    }

    # Configuraci√≥n de coherencia
    COHERENCE_CONFIG = {
        "min_semantic_similarity": 0.5,
        "min_relevance_score": 0.6,
        "min_connector_density": 0.1,
        "min_entity_consistency": 0.7,
    }

    @classmethod
    def get_model_paths(cls) -> Dict[str, str]:
        """Obtener rutas de modelos"""
        return {"main_model": cls.MAIN_MODEL}

    @classmethod
    def get_quality_thresholds(cls) -> Dict[str, float]:
        """Obtener umbrales de calidad"""
        return cls.QUALITY_THRESHOLDS.copy()

    @classmethod
    def get_quality_weights(cls) -> Dict[str, float]:
        """Obtener pesos de calidad"""
        return cls.QUALITY_WEIGHTS.copy()

    @classmethod
    def create_directories(cls):
        """Crear directorios necesarios"""
        directories = [cls.LOG_DIR, cls.RESULTS_DIR, cls.MODELS_DIR]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    @classmethod
    def get_log_file_path(cls, evaluator_name: str) -> str:
        """Obtener ruta del archivo de log"""
        return os.path.join(cls.LOG_DIR, f"{evaluator_name}.log")

    @classmethod
    def get_results_file_path(cls, evaluator_name: str) -> str:
        """Obtener ruta del archivo de resultados"""
        return os.path.join(cls.RESULTS_DIR, f"{evaluator_name}_results.json")

    @classmethod
    def get_visualization_path(cls, name: str) -> str:
        """Obtener ruta para visualizaciones"""
        return os.path.join(cls.RESULTS_DIR, f"{name}.png")


class PipelineConfig:
    """Configuraci√≥n espec√≠fica para el pipeline de evaluaci√≥n"""

    # Configuraci√≥n de procesamiento por lotes
    BATCH_SIZE = 10
    MAX_WORKERS = 4

    # Configuraci√≥n de cache
    ENABLE_CACHE = True
    CACHE_TTL = 3600  # 1 hora

    # Configuraci√≥n de evaluaci√≥n de conversaciones
    MAX_CONVERSATION_LENGTH = 50
    MIN_CONVERSATION_LENGTH = 2

    # Configuraci√≥n de reportes
    GENERATE_REPORTS = True
    REPORT_FORMATS = ["json", "html", "csv"]

    @classmethod
    def get_pipeline_settings(cls) -> Dict[str, Any]:
        """Obtener configuraci√≥n del pipeline"""
        return {
            "batch_size": cls.BATCH_SIZE,
            "max_workers": cls.MAX_WORKERS,
            "enable_cache": cls.ENABLE_CACHE,
            "cache_ttl": cls.CACHE_TTL,
            "max_conversation_length": cls.MAX_CONVERSATION_LENGTH,
            "min_conversation_length": cls.MIN_CONVERSATION_LENGTH,
            "generate_reports": cls.GENERATE_REPORTS,
            "report_formats": cls.REPORT_FORMATS,
        }


class BenchmarkConfig:
    """Configuraci√≥n espec√≠fica para benchmarks de rendimiento"""

    # Configuraci√≥n de pruebas
    TEST_SIZES = [10, 50, 100, 500, 1000]
    REPETITIONS = 3

    # Configuraci√≥n de m√©tricas
    METRICS_TO_TRACK = [
        "execution_time",
        "memory_usage",
        "cpu_usage",
        "accuracy",
        "throughput",
    ]

    # Configuraci√≥n de visualizaci√≥n
    PLOT_STYLES = {"figure_size": (12, 8), "dpi": 300, "style": "seaborn-v0_8"}

    @classmethod
    def get_benchmark_settings(cls) -> Dict[str, Any]:
        """Obtener configuraci√≥n del benchmark"""
        return {
            "test_sizes": cls.TEST_SIZES,
            "repetitions": cls.REPETITIONS,
            "metrics_to_track": cls.METRICS_TO_TRACK,
            "plot_styles": cls.PLOT_STYLES,
        }


# Configuraci√≥n de entorno
ENVIRONMENT_CONFIG = {
    "development": {
        "log_level": "DEBUG",
        "save_intermediate_results": True,
        "enable_profiling": True,
    },
    "production": {
        "log_level": "INFO",
        "save_intermediate_results": False,
        "enable_profiling": False,
    },
    "testing": {
        "log_level": "WARNING",
        "save_intermediate_results": True,
        "enable_profiling": False,
    },
}


def get_environment_config(env: str = "development") -> Dict[str, Any]:
    """Obtener configuraci√≥n seg√∫n el entorno"""
    return ENVIRONMENT_CONFIG.get(env, ENVIRONMENT_CONFIG["development"])


def validate_config():
    """Validar configuraci√≥n"""
    # Verificar que los pesos sumen 1.0
    weights = EvaluationConfig.get_quality_weights()
    total_weight = sum(weights.values())
    if abs(total_weight - 1.0) > 0.01:
        raise ValueError(
            f"Los pesos de calidad deben sumar 1.0, actual: {total_weight}"
        )

    # Verificar umbrales v√°lidos
    thresholds = EvaluationConfig.get_quality_thresholds()
    for name, threshold in thresholds.items():
        if not 0 <= threshold <= 1:
            raise ValueError(
                f"Umbral {name} debe estar entre 0 y 1, actual: {threshold}"
            )

    print("‚úÖ Configuraci√≥n validada correctamente")


if __name__ == "__main__":
    # Crear directorios
    EvaluationConfig.create_directories()

    # Validar configuraci√≥n
    validate_config()

    # Mostrar configuraci√≥n
    print("üìã Configuraci√≥n del Sistema de Evaluaci√≥n:")
    print(f"  Umbrales: {EvaluationConfig.get_quality_thresholds()}")
    print(f"  Pesos: {EvaluationConfig.get_quality_weights()}")
    print(f"  Modelos: {EvaluationConfig.get_model_paths()}")
    print(
        f"  Directorios creados: {EvaluationConfig.LOG_DIR}, {EvaluationConfig.RESULTS_DIR}"
    )
