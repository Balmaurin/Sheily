# Sistema de Evaluaci√≥n - Shaili AI

## üìÅ Estructura del Sistema de Evaluaci√≥n

```
evaluation/
‚îú‚îÄ‚îÄ diversity.py                    # üéØ Evaluador de diversidad ling√º√≠stica (200+ l√≠neas)
‚îú‚îÄ‚îÄ toxicity.py                     # ‚ö†Ô∏è Evaluador de toxicidad (250+ l√≠neas)
‚îú‚îÄ‚îÄ coherence.py                    # üîó Evaluador de coherencia (280+ l√≠neas)
‚îú‚îÄ‚îÄ pipeline.py                     # üîÑ Pipeline de evaluaci√≥n de calidad (235 l√≠neas)
‚îú‚îÄ‚îÄ performance_benchmark.py        # üìä Benchmark de rendimiento (250 l√≠neas)
‚îú‚îÄ‚îÄ config.py                       # ‚öôÔ∏è Configuraci√≥n del sistema (250 l√≠neas)
‚îú‚îÄ‚îÄ setup.py                        # üöÄ Script de instalaci√≥n (288 l√≠neas)
‚îú‚îÄ‚îÄ __init__.py                     # üì¶ Paquete Python funcional (300+ l√≠neas)
‚îú‚îÄ‚îÄ test_evaluation_system.py       # üß™ Sistema de pruebas completo (400+ l√≠neas)
‚îî‚îÄ‚îÄ README.md                       # üìñ Esta documentaci√≥n
```

## üìä Estad√≠sticas del Sistema

### üìÑ Archivos: 9
### üíª L√≠neas de c√≥digo: 2,000+
### üêç Python: 2,000+ l√≠neas
### üéØ Evaluadores: 5 clases principales
### ‚úÖ Estado: Completamente funcional

## üéØ Componentes del Sistema

### 1. **DiversityEvaluator** (`diversity.py`)

#### **Funci√≥n Principal:**
Evaluar la diversidad ling√º√≠stica de las respuestas generadas por el modelo de IA.

#### **M√©tricas Implementadas:**
- **Riqueza L√©xica**:
  - Type-Token Ratio (TTR)
  - √çndice de Guiraud
  - √çndice de Herdan
- **Complejidad Sint√°ctica**:
  - Longitud promedio de oraciones
  - Variedad de estructuras gramaticales
  - Complejidad de palabras
- **Variaci√≥n Sem√°ntica**:
  - Entrop√≠a de n-gramas (bigramas y trigramas)
  - Dispersi√≥n sem√°ntica

#### **Dependencias:**
```python
import numpy as np
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.util import ngrams
from typing import List, Dict, Any
import re
import logging
```

#### **Instalaci√≥n de Dependencias:**
```bash
pip install numpy nltk
python -m nltk.downloader punkt stopwords
```

#### **Uso:**
```python
from evaluation.diversity import DiversityEvaluator

evaluator = DiversityEvaluator()
diversity_metrics = evaluator.evaluate_diversity(texto)
print(f"Diversidad: {diversity_metrics['diversity_score']}")
```

### 2. **ToxicityEvaluator** (`toxicity.py`)

#### **Funci√≥n Principal:**
Detectar y evaluar contenido t√≥xico o inapropiado en las respuestas del modelo.

#### **Categor√≠as de Toxicidad:**
- **Insultos**: Palabras ofensivas directas
- **Discriminaci√≥n**: Lenguaje discriminatorio
- **Sexismo**: Contenido sexista
- **Violencia**: Referencias violentas
- **Odio**: Expresiones de odio

#### **M√©tricas Implementadas:**
- **Detecci√≥n de Lenguaje T√≥xico**:
  - An√°lisis l√©xico por categor√≠as
  - Puntuaci√≥n de severidad
  - Detecci√≥n de negaciones
- **An√°lisis Contextual**:
  - Patrones agresivos
  - Uso de may√∫sculas excesivo
  - An√°lisis de entidades

#### **Dependencias:**
```python
import re
import numpy as np
from typing import List, Dict, Union, Any
import json
import os
import logging
```

#### **Uso:**
```python
from evaluation.toxicity import ToxicityEvaluator

evaluator = ToxicityEvaluator()
toxicity_result = evaluator.evaluate_toxicity(texto)
print(f"Es t√≥xico: {toxicity_result['is_toxic']}")
```

### 3. **CoherenceEvaluator** (`coherence.py`)

#### **Funci√≥n Principal:**
Evaluar la coherencia sem√°ntica y l√≥gica de las respuestas del modelo.

#### **M√©tricas Implementadas:**
- **Coherencia Sem√°ntica**: Similitud TF-IDF entre consulta y respuesta
- **Relevancia**: Presencia de palabras clave de la consulta
- **Estructura L√≥gica**: An√°lisis de conectores l√≥gicos
- **Consistencia**: Verificaci√≥n de entidades y coherencia interna

#### **Dependencias:**
```python
import numpy as np
from typing import Dict, Any, List
import re
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
```

#### **Instalaci√≥n de Dependencias:**
```bash
pip install scikit-learn
```

#### **Uso:**
```python
from evaluation.coherence import CoherenceEvaluator

evaluator = CoherenceEvaluator()
coherence_score = evaluator.calculate_coherence(query, response)
print(f"Coherencia: {coherence_score}")
```

### 4. **QualityEvaluationPipeline** (`pipeline.py`)

#### **Funci√≥n Principal:**
Pipeline completo para evaluar la calidad de las respuestas del modelo, combinando m√∫ltiples m√©tricas.

#### **Caracter√≠sticas:**
- **Evaluaci√≥n Integrada**: Combina coherencia, diversidad y toxicidad
- **Puntuaci√≥n Compuesta**: Peso configurable para cada m√©trica
- **Umbrales de Calidad**: Configuraci√≥n de est√°ndares m√≠nimos
- **Logging Detallado**: Registro de todas las evaluaciones
- **Evaluaci√≥n de Conversaciones**: An√°lisis de conversaciones completas

#### **M√©tricas Combinadas:**
- **Coherencia** (40%): Relevancia y l√≥gica
- **Diversidad** (30%): Variedad ling√º√≠stica
- **Toxicidad** (30%): Ausencia de contenido inapropiado

#### **Uso:**
```python
from evaluation.pipeline import QualityEvaluationPipeline

pipeline = QualityEvaluationPipeline()
evaluation = pipeline.evaluate_response(query, response, domain="ciencia")
print(f"Puntuaci√≥n: {evaluation['composite_score']}")
```

### 5. **PerformanceBenchmark** (`performance_benchmark.py`)

#### **Funci√≥n Principal:**
Evaluar el rendimiento y eficiencia de los componentes del sistema de IA.

#### **Componentes Evaluados:**
- **Clustering Sem√°ntico**: `AdvancedSemanticClustering`
- **Optimizaci√≥n de Adapters**: `DomainAdapterOptimizer`
- **Expansi√≥n de Dominios**: `DomainExpansionEngine`

#### **M√©tricas de Rendimiento:**
- **Tiempo de Ejecuci√≥n**: Duraci√≥n de operaciones
- **Uso de Memoria**: Consumo de RAM/GPU
- **Escalabilidad**: Rendimiento con diferentes tama√±os de datos
- **Coherencia**: Calidad de resultados

#### **Uso:**
```python
from evaluation.performance_benchmark import PerformanceBenchmark

benchmark = PerformanceBenchmark()
results = benchmark.run_comprehensive_benchmark()
print("Benchmark completado")
```

### 6. **EvaluationConfig** (`config.py`)

#### **Funci√≥n Principal:**
Configuraci√≥n centralizada del sistema de evaluaci√≥n.

#### **Caracter√≠sticas:**
- **Umbrales de Calidad**: Configuraci√≥n de est√°ndares m√≠nimos
- **Pesos de M√©tricas**: Configuraci√≥n de importancia relativa
- **Configuraci√≥n de Logging**: Niveles y formatos de logs
- **Configuraci√≥n de Entorno**: Desarrollo, producci√≥n, testing

#### **Uso:**
```python
from evaluation.config import EvaluationConfig

# Obtener configuraci√≥n
thresholds = EvaluationConfig.get_quality_thresholds()
weights = EvaluationConfig.get_quality_weights()

# Crear directorios
EvaluationConfig.create_directories()
```

### 7. **Setup Script** (`setup.py`)

#### **Funci√≥n Principal:**
Script de instalaci√≥n autom√°tica del sistema de evaluaci√≥n.

#### **Caracter√≠sticas:**
- **Instalaci√≥n de Dependencias**: Autom√°tica con pip
- **Descarga de Recursos**: Modelos NLP y datasets
- **Creaci√≥n de Directorios**: Estructura de carpetas
- **Pruebas de Importaci√≥n**: Verificaci√≥n de m√≥dulos
- **Pruebas B√°sicas**: Validaci√≥n de funcionalidad

#### **Uso:**
```bash
# Instalaci√≥n completa
python evaluation/setup.py
```

### 8. **Paquete Python** (`__init__.py`)

#### **Funci√≥n Principal:**
Transforma la carpeta `evaluation` en un paquete Python funcional.

#### **Caracter√≠sticas:**
- **API Unificada**: Funciones de conveniencia para evaluaci√≥n r√°pida
- **Instancias Globales**: Gesti√≥n autom√°tica de evaluadores
- **Configuraci√≥n Centralizada**: Acceso f√°cil a configuraciones
- **Validaci√≥n Autom√°tica**: Verificaci√≥n del sistema al importar
- **Reportes**: Generaci√≥n autom√°tica de reportes de evaluaci√≥n

#### **Uso:**
```python
import evaluation

# Evaluaci√≥n r√°pida
diversity_score = evaluation.evaluate_diversity(texto)
toxicity_score = evaluation.evaluate_toxicity(texto)
coherence_score = evaluation.evaluate_coherence(query, response)

# Evaluaci√≥n completa
quality_result = evaluation.evaluate_quality(query, response, domain="ciencia")

# Validaci√≥n del sistema
validation = evaluation.validate_evaluation_system()
```

### 9. **Sistema de Pruebas** (`test_evaluation_system.py`)

#### **Funci√≥n Principal:**
Sistema completo de pruebas para validar todos los componentes.

#### **Caracter√≠sticas:**
- **Pruebas Unitarias**: Validaci√≥n individual de cada evaluador
- **Pruebas de Integraci√≥n**: Verificaci√≥n del pipeline completo
- **Pruebas de Configuraci√≥n**: Validaci√≥n de configuraciones
- **Reportes Autom√°ticos**: Generaci√≥n de reportes de pruebas
- **Validaci√≥n del Paquete**: Pruebas de importaci√≥n y API

#### **Uso:**
```bash
# Ejecutar todas las pruebas
python evaluation/test_evaluation_system.py
```

## üîß Instalaci√≥n y Configuraci√≥n

### 1. **Instalaci√≥n Autom√°tica (Recomendada)**
```bash
# Ejecutar script de instalaci√≥n
python evaluation/setup.py
```

### 2. **Instalaci√≥n Manual**
```bash
# Instalar dependencias
pip install numpy nltk scikit-learn matplotlib

# Descargar recursos NLTK
python -m nltk.downloader punkt stopwords

# Crear directorios
mkdir -p logs/evaluation results/evaluation models/evaluation datasets/test
```

### 3. **Verificaci√≥n de Instalaci√≥n**
```bash
# Probar todos los evaluadores
python evaluation/diversity.py
python evaluation/toxicity.py
python evaluation/coherence.py
python evaluation/pipeline.py

# Ejecutar pruebas completas
python evaluation/test_evaluation_system.py
```

## üöÄ Ejecuci√≥n del Sistema

### **Ejecuci√≥n Individual de Evaluadores**

#### 1. Evaluaci√≥n de Diversidad
```bash
cd evaluation
python diversity.py
```

#### 2. Evaluaci√≥n de Toxicidad
```bash
cd evaluation
python toxicity.py
```

#### 3. Evaluaci√≥n de Coherencia
```bash
cd evaluation
python coherence.py
```

#### 4. Pipeline Completo
```bash
cd evaluation
python pipeline.py
```

#### 5. Benchmark de Rendimiento
```bash
cd evaluation
python performance_benchmark.py
```

### **Ejecuci√≥n del Sistema de Pruebas**
```bash
cd evaluation
python test_evaluation_system.py
```

## üìä Configuraci√≥n de M√©tricas

### **Umbrales de Calidad (Pipeline)**
```python
pipeline = QualityEvaluationPipeline(
    coherence_weight=0.4,      # Peso de coherencia
    diversity_weight=0.3,      # Peso de diversidad
    toxicity_weight=0.3,       # Peso de toxicidad
    coherence_threshold=0.6,   # Umbral m√≠nimo de coherencia
    diversity_threshold=0.5,   # Umbral m√≠nimo de diversidad
    toxicity_threshold=0.3     # Umbral m√°ximo de toxicidad
)
```

### **Configuraci√≥n de Logging**
```python
# Los evaluadores crean autom√°ticamente:
# - logs/evaluation/quality_evaluation.log
# - logs/performance/performance_benchmark.log
```

## üîç Estructura de Datos

### **Resultado de Evaluaci√≥n de Diversidad**
```python
{
    'diversity_score': 0.75,
    'lexical_metrics': {
        'type_token_ratio': 0.65,
        'guiraud_index': 12.3,
        'herdan_index': 0.45
    },
    'syntactic_metrics': {
        'avg_sentence_length': 15.2,
        'max_syntax_depth': 4,
        'pos_diversity': 0.78,
        'word_complexity': 5.8
    },
    'semantic_metrics': {
        'bigram_entropy': 8.5,
        'trigram_entropy': 12.3,
        'semantic_dispersion': 0.67
    }
}
```

### **Resultado de Evaluaci√≥n de Toxicidad**
```python
{
    'is_toxic': False,
    'toxicity_score': 0.1,
    'base_toxicity': 0.05,
    'context_penalty': 0.03,
    'uppercase_penalty': 0.02,
    'toxic_categories': [],
    'context_analysis': {
        'entidades': ['plantas', 'energ√≠a'],
        'dependencias_agresivas': 0.05,
        'pattern_matches': [],
        'avg_word_length': 6.2,
        'uppercase_ratio': 0.02
    }
}
```

### **Resultado del Pipeline**
```python
{
    'composite_score': 0.82,
    'passes_quality': True,
    'metrics': {
        'coherence': {
            'score': 0.85,
            'passes_threshold': True
        },
        'diversity': {
            'score': 0.75,
            'passes_threshold': True
        },
        'toxicity': {
            'score': 0.90,
            'passes_threshold': True
        }
    },
    'domain': 'ciencia'
}
```

## üõ†Ô∏è Desarrollo y Extensi√≥n

### **Agregar Nuevas M√©tricas**

#### 1. Crear Nuevo Evaluador
```python
class CustomEvaluator:
    def __init__(self):
        # Inicializaci√≥n
        pass
    
    def evaluate(self, text: str) -> Dict[str, Any]:
        # L√≥gica de evaluaci√≥n
        return {'custom_score': 0.85}
```

#### 2. Integrar en Pipeline
```python
# En pipeline.py
from .custom_evaluator import CustomEvaluator

class QualityEvaluationPipeline:
    def __init__(self):
        self.custom_evaluator = CustomEvaluator()
        # ... resto del c√≥digo
```

### **Personalizar Umbrales**
```python
# Crear configuraci√≥n personalizada
config = {
    'coherence_threshold': 0.7,
    'diversity_threshold': 0.6,
    'toxicity_threshold': 0.2,
    'weights': {
        'coherence': 0.5,
        'diversity': 0.3,
        'toxicity': 0.2
    }
}
```

## üìà Monitoreo y Logs

### **Ubicaci√≥n de Logs**
- **Pipeline**: `logs/evaluation/`
- **Benchmark**: `logs/performance/`
- **Pruebas**: `results/evaluation/`

### **Tipos de Logs**
- **Evaluaciones**: Resultados detallados de cada evaluaci√≥n
- **Rendimiento**: M√©tricas de tiempo y memoria
- **Errores**: Problemas durante la evaluaci√≥n
- **Pruebas**: Reportes de validaci√≥n del sistema

### **Visualizaci√≥n de Resultados**
```python
# El benchmark genera autom√°ticamente:
# - performance_visualization.png
# - performance_results.json
# - test_report.json
```

## üîß Troubleshooting

### **Problemas Comunes**

#### 1. Error de Importaci√≥n de NLTK
```bash
# Soluci√≥n: Descargar recursos NLTK
python -m nltk.downloader punkt stopwords
```

#### 2. Error de Dependencias Faltantes
```bash
# Instalar dependencias
pip install numpy nltk scikit-learn matplotlib
```

#### 3. Error de Configuraci√≥n
```bash
# Validar configuraci√≥n
python evaluation/config.py
```

#### 4. Error de Memoria
```bash
# Reducir tama√±o de datos de prueba
test_texts = evaluator.generate_test_data(num_texts=50)
```

## üìù Notas de Implementaci√≥n

### **Caracter√≠sticas Destacadas**
- ‚úÖ **Evaluaci√≥n Multi-dimensional**: Combina m√∫ltiples m√©tricas
- ‚úÖ **Configuraci√≥n Flexible**: Umbrales y pesos ajustables
- ‚úÖ **Logging Detallado**: Registro completo de evaluaciones
- ‚úÖ **Visualizaci√≥n**: Gr√°ficos autom√°ticos de rendimiento
- ‚úÖ **Extensibilidad**: F√°cil agregar nuevas m√©tricas
- ‚úÖ **Paquete Python**: API unificada y f√°cil de usar
- ‚úÖ **Sistema de Pruebas**: Validaci√≥n completa del sistema
- ‚úÖ **Sin Dependencias Externas**: No requiere SpaCy o transformers

### **Mejoras Implementadas**
- üîß **Eliminaci√≥n de SpaCy**: Reemplazado con an√°lisis basado en regex y NLTK
- üîß **Eliminaci√≥n de Transformers**: Reemplazado con TF-IDF para similitud sem√°ntica
- üîß **Mejor Manejo de Errores**: Validaci√≥n robusta y fallbacks
- üîß **API Unificada**: Funciones de conveniencia en el paquete
- üîß **Sistema de Pruebas**: Validaci√≥n completa de todos los componentes

### **Limitaciones Actuales**
- ‚ö†Ô∏è **An√°lisis Sint√°ctico**: Limitado sin SpaCy (usando patrones simples)
- ‚ö†Ô∏è **Similitud Sem√°ntica**: Basada en TF-IDF en lugar de embeddings
- ‚ö†Ô∏è **Recursos de NLP**: Requiere descarga de recursos NLTK

### **Mejoras Futuras**
- üìã **M√©tricas Adicionales**: Fluidez, precisi√≥n, completitud
- üìã **Evaluaci√≥n Autom√°tica**: Integraci√≥n con CI/CD
- üìã **Dashboard Web**: Interfaz para visualizar resultados
- üìã **Optimizaci√≥n de Rendimiento**: Caching y procesamiento paralelo
- üìã **An√°lisis Sint√°ctico Avanzado**: Integraci√≥n con herramientas m√°s sofisticadas

## üéØ Integraci√≥n con Shaili AI

### **Uso en el Sistema Principal**
```python
# En el sistema de IA
import evaluation

# Evaluar respuesta antes de enviar
quality_check = evaluation.evaluate_quality(query, response)

if quality_check['passes_quality']:
    # Enviar respuesta
    send_response(response)
else:
    # Generar respuesta alternativa
    generate_alternative_response()
```

### **Monitoreo Continuo**
```python
# Evaluar calidad de conversaciones completas
conversation_quality = evaluation.evaluate_conversation(conversation)
print(f"Calidad promedio: {conversation_quality['mean_composite_score']}")
```

### **Validaci√≥n del Sistema**
```python
# Validar que todo funcione correctamente
validation = evaluation.validate_evaluation_system()
if validation['overall_status']:
    print("‚úÖ Sistema de evaluaci√≥n funcionando correctamente")
else:
    print("‚ùå Problemas detectados en el sistema de evaluaci√≥n")
```

## üöÄ Instalaci√≥n R√°pida

### **Opci√≥n 1: Instalaci√≥n Autom√°tica (Recomendada)**
```bash
# Clonar o navegar al directorio del proyecto
cd /path/to/shaili-ai

# Ejecutar instalaci√≥n autom√°tica
python evaluation/setup.py
```

### **Opci√≥n 2: Instalaci√≥n Manual**
```bash
# 1. Instalar dependencias
pip install numpy nltk scikit-learn matplotlib

# 2. Descargar recursos NLP
python -m nltk.downloader punkt stopwords

# 3. Crear directorios
mkdir -p logs/evaluation results/evaluation models/evaluation datasets/test

# 4. Validar configuraci√≥n
python evaluation/config.py
```

### **Verificaci√≥n de Instalaci√≥n**
```bash
# Probar todos los evaluadores
python evaluation/diversity.py
python evaluation/toxicity.py
python evaluation/coherence.py
python evaluation/pipeline.py

# Ejecutar pruebas completas
python evaluation/test_evaluation_system.py
```

## üìä Resultados Esperados

### **Ejemplo de Salida del Pipeline**
```json
{
  "composite_score": 0.82,
  "passes_quality": true,
  "metrics": {
    "coherence": {"score": 0.85, "passes_threshold": true},
    "diversity": {"score": 0.75, "passes_threshold": true},
    "toxicity": {"score": 0.90, "passes_threshold": true}
  }
}
```

### **Ejemplo de Salida de Pruebas**
```json
{
  "test_summary": {
    "total_tests": 7,
    "passed_tests": 7,
    "failed_tests": 0
  },
  "overall_status": true
}
```

---

**Nota**: Este sistema de evaluaci√≥n es fundamental para garantizar la calidad de las respuestas del modelo de IA. Se recomienda ejecutar evaluaciones regularmente y ajustar los umbrales seg√∫n las necesidades espec√≠ficas del proyecto. El sistema est√° completamente funcional y no requiere dependencias externas complejas.
